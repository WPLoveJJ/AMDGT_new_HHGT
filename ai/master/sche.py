import requests
import json
import urllib.parse
import schedule
import time
import os
import re
from datetime import datetime, date, timedelta
import logging

# -------------------------- 1. åŸºç¡€é…ç½® --------------------------
DINGTALK_CONFIG = {
    "app_key": "dingczeweiukv9kue2gv",
    "app_secret": "BC11ILonRquetv-aTv6lrfUlqWHjDrikSQN9NWHhxRHVz8xYQGcnLgtL6h1SPiPU",
    "config_base_id": "pYLaezmVNev7pRZ9t4oxG9aQWrMqPxX6",
    "config_sheet_name": "é…ç½®è¡¨",
    "operator_id": "xYLFMT7vpx2nLD5iiW81omAiEiE",
    "token_cache_file": "dingtalk_stretch_token_cache.json"
}

# -------------------------- 2. æ—¥å¿—é…ç½® --------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('stretch_scheduler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# -------------------------- 3. å·¥å…·å‡½æ•° --------------------------
def safe_strip(value):
    """å®‰å…¨å¤„ç†å­—ç¬¦ä¸²ï¼Œå…¼å®¹Noneå’Œéå­—ç¬¦ä¸²ç±»å‹"""
    if value is None:
        return ""
    return str(value).strip()


def parse_dingtalk_table_url(table_url):
    """è§£æé’‰é’‰å¤šç»´è¡¨é“¾æ¥ï¼Œæå–base_idå’Œsheet_id"""
    try:
        parsed = urllib.parse.urlparse(table_url)
        query_params = urllib.parse.parse_qs(parsed.query)

        # 1. ä¼˜å…ˆä»å¸¸è§„æŸ¥è¯¢å‚æ•°æå–ï¼ˆæ—§æ ¼å¼ï¼‰
        base_id = safe_strip(query_params.get("baseId", [None])[0])
        sheet_id = safe_strip(query_params.get("sheetId", [None])[0])

        # 2. å¤„ç†alidocs.dingtalk.comæ–°æ ¼å¼
        if not (base_id and sheet_id) and parsed.netloc == "alidocs.dingtalk.com":
            # ä»iframeQueryä¸­è§£æsheetId
            iframe_query = query_params.get("iframeQuery", [None])[0]
            if iframe_query:
                iframe_params = urllib.parse.parse_qs(iframe_query)
                sheet_id = safe_strip(iframe_params.get("sheetId", [None])[0])

            # ä»è·¯å¾„æå–baseId
            path_parts = parsed.path.split("/")
            if len(path_parts) >= 4 and path_parts[2] == "nodes":
                base_id = path_parts[3]

        # 3. å…¼å®¹æ—§æ ¼å¼ï¼ˆ/bases/{baseId}/sheets/{sheetId}è·¯å¾„ï¼‰
        if not (base_id and sheet_id):
            path_parts = parsed.path.split("/")
            if len(path_parts) >= 5 and path_parts[3] == "bases":
                base_id = path_parts[4]
                sheet_id = path_parts[6] if len(path_parts) >= 7 else None

        if base_id and sheet_id:
            logger.info(f"âœ… è§£æè¡¨é“¾æ¥æˆåŠŸï¼šbase_id={base_id}, sheet_id={sheet_id}")
            return {"base_id": base_id, "sheet_id": sheet_id}
        else:
            logger.error(f"âŒ æ— æ³•è§£æè¡¨é“¾æ¥ï¼Œç¼ºå¤±å‚æ•°ï¼šbase_id={base_id}, sheet_id={sheet_id} | é“¾æ¥={table_url}")
            return None
    except Exception as e:
        logger.error(f"âŒ è§£æè¡¨é“¾æ¥å‡ºé”™ï¼š{e} | é“¾æ¥={table_url}")
        return None


# -------------------------- 4. é’‰é’‰Tokenç®¡ç† --------------------------
def load_cached_token():
    """åŠ è½½Tokenç¼“å­˜"""
    cache_file = DINGTALK_CONFIG["token_cache_file"]
    try:
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)

            expire_time = datetime.fromisoformat(cache["expire_time"])
            if datetime.now() < expire_time - timedelta(minutes=5):
                logger.info("ğŸ”„ ä½¿ç”¨ç¼“å­˜çš„Token")
                return cache["access_token"]
            logger.info("â° Tokenå³å°†è¿‡æœŸï¼Œéœ€é‡æ–°è·å–")
    except Exception as e:
        logger.warning(f"âš ï¸ è¯»å–Tokenç¼“å­˜å¤±è´¥ï¼š{e}")
    return None


def save_token_to_cache(access_token, expires_in=7200):
    """ä¿å­˜Tokenåˆ°ç¼“å­˜"""
    cache_file = DINGTALK_CONFIG["token_cache_file"]
    try:
        expire_time = datetime.now() + timedelta(seconds=expires_in)
        cache_data = {
            "access_token": access_token,
            "expire_time": expire_time.isoformat(),
            "created_time": datetime.now().isoformat()
        }
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Tokenå·²ç¼“å­˜ï¼Œè¿‡æœŸæ—¶é—´ï¼š{expire_time.strftime('%Y-%m-%d %H:%M:%S')}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜Tokenç¼“å­˜å¤±è´¥ï¼š{e}")


def get_dingtalk_access_token():
    """è·å–é’‰é’‰Access Tokenï¼ˆå¸¦ç¼“å­˜å’Œé‡è¯•æœºåˆ¶ï¼‰"""
    cached_token = load_cached_token()
    if cached_token:
        return cached_token

    logger.info("ğŸ”„ğŸ”„ é‡æ–°è·å–Token...")
    url = "https://api.dingtalk.com/v1.0/oauth2/accessToken"
    headers = {"Content-Type": "application/json"}
    payload = {
        "appKey": DINGTALK_CONFIG["app_key"],
        "appSecret": DINGTALK_CONFIG["app_secret"]
    }

    # åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯
    session = requests.Session()
    retry_strategy = requests.adapters.Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["POST"]
    )
    adapter = requests.adapters.HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    try:
        response = session.post(url, headers=headers, json=payload, timeout=15)
        response.raise_for_status()
        data = response.json()
        access_token = data.get("accessToken")
        expires_in = data.get("expireIn", 7200)

        if access_token:
            save_token_to_cache(access_token, expires_in)
            logger.info("âœ… Tokenè·å–æˆåŠŸ")
            return access_token
        logger.error("âŒâŒ å“åº”ä¸­æ— Token")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒâŒ è·å–Tokenå¤±è´¥ï¼ˆé‡è¯•åï¼‰ï¼š{e}")
    except Exception as e:
        logger.error(f"âŒâŒ è·å–Tokenå‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
    return None


# -------------------------- 5. è¯»å–é’‰é’‰é…ç½®è¡¨ï¼ˆæ”¯æŒå¤šæ¨é€æ—¶é—´ï¼‰ --------------------------
def get_task_configs():
    """ä»é’‰é’‰é…ç½®è¡¨è·å–ä»»åŠ¡ï¼ˆæ”¯æŒæ¨é€æ—¶é—´å¤šå€¼ï¼Œå¦‚8:00,10:00ï¼‰"""
    logger.info("ğŸ”„ è¯»å–é’‰é’‰é…ç½®è¡¨...")
    access_token = get_dingtalk_access_token()
    if not access_token:
        logger.error("âŒ æ— Tokenï¼Œæ— æ³•è¯»å–é…ç½®è¡¨")
        return []

    base_id = DINGTALK_CONFIG["config_base_id"]
    sheet_name = urllib.parse.quote(DINGTALK_CONFIG["config_sheet_name"])
    url = f"https://api.dingtalk.com/v1.0/notable/bases/{base_id}/sheets/{sheet_name}/records"
    headers = {
        "x-acs-dingtalk-access-token": access_token,
        "Content-Type": "application/json"
    }
    params = {"maxResults": 100, "operatorId": DINGTALK_CONFIG["operator_id"]}

    # ä¸‰æ¬¡é‡è¯•é…ç½®
    max_retries = 3
    retry_interval = 2
    retry_count = 0

    while retry_count < max_retries:
        try:
            logger.info(f"ğŸ”„ ç¬¬{retry_count + 1}æ¬¡è¯·æ±‚é…ç½®è¡¨API")
            response = requests.get(
                url,
                headers=headers,
                params=params,
                timeout=15,
                verify=True
            )
            response.raise_for_status()
            data = response.json()
            records = data.get("records", [])

            if not records:
                logger.warning("âš ï¸ é…ç½®è¡¨ä¸­æ— è®°å½•")
                return []

            task_configs = []
            for idx, record in enumerate(records, 1):
                fields = record.get("fields", {})
                record_id = safe_strip(record.get("recordId") or record.get("id"))

                # å¤„ç†å‹¾é€‰æ¡†ç±»å‹"æ˜¯å¦å¯ç”¨"
                is_enabled_checkbox = fields.get("æ˜¯å¦å¯ç”¨", False)
                is_enabled = "å·²å¯ç”¨" if is_enabled_checkbox else "æœªå¯ç”¨"

                # è§£æwebhookï¼ˆä»linkå­—æ®µå–URLï¼‰
                webhook_field = fields.get("webhook", {})
                webhook_title = safe_strip(webhook_field.get("text", ""))
                webhook_url = safe_strip(webhook_field.get("link", ""))

                # è§£æè¡¨é“¾æ¥ï¼ˆä»linkå­—æ®µå–URLï¼‰
                table_url_field = fields.get("è¡¨é“¾æ¥", {})
                table_url = safe_strip(table_url_field.get("link", ""))

                # è§£æå®šæ—¶ç±»å‹
                cron_type_field = fields.get("å®šæ—¶ç±»å‹", {})
                cron_type = safe_strip(cron_type_field.get("name", ""))

                # è·å–@äººå­—æ®µçš„åç§°
                at_field_name = safe_strip(fields.get("@äººå­—æ®µ", ""))

                # å¤„ç†"æœ€æ–°å®Œæˆæ—¥æœŸ"å­—æ®µ
                latest_exec_date_field = safe_strip(fields.get("æœ€æ–°å®Œæˆæ—¥æœŸ", ""))
                latest_exec_date = ""
                if latest_exec_date_field:
                    try:
                        clean_date_str = re.sub(r'[\s\xa0]+', ' ', latest_exec_date_field).strip()
                        clean_date_str = clean_date_str.replace('ï¼š', ':')

                        if clean_date_str.isdigit():
                            timestamp_seconds = int(clean_date_str) / 1000
                            date_obj = datetime.fromtimestamp(timestamp_seconds)
                            latest_exec_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
                            logger.info(f"âœ… è½¬æ¢æœ€æ–°å®Œæˆæ—¥æœŸæ—¶é—´æˆ³: {clean_date_str} -> {latest_exec_date}")
                        else:
                            formats = [
                                "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M",
                                "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M",
                                "%Y-%m-%d", "%Y/%m/%d"
                            ]

                            date_obj = None
                            for fmt in formats:
                                try:
                                    date_obj = datetime.strptime(clean_date_str, fmt)
                                    break
                                except ValueError:
                                    continue

                            if not date_obj:
                                parts = clean_date_str.split(' ')
                                date_part = parts[0] if len(parts) >= 1 else None
                                time_part = parts[1] if len(parts) >= 2 else None

                                if date_part:
                                    date_formats = ["%Y-%m-%d", "%Y/%m/%d"]
                                    for df in date_formats:
                                        try:
                                            date_obj = datetime.strptime(date_part, df)
                                            break
                                        except ValueError:
                                            continue

                                if time_part and date_obj:
                                    time_formats = ["%H:%M:%S", "%H:%M"]
                                    for tf in time_formats:
                                        try:
                                            time_obj = datetime.strptime(time_part, tf)
                                            date_obj = date_obj.replace(
                                                hour=time_obj.hour,
                                                minute=time_obj.minute,
                                                second=time_obj.second
                                            )
                                            break
                                        except ValueError:
                                            continue

                            if not date_obj:
                                raise ValueError(f"æ‰€æœ‰æ ¼å¼å‡åŒ¹é…å¤±è´¥: {clean_date_str}")

                            latest_exec_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
                            logger.info(f"âœ… è½¬æ¢æœ€æ–°å®Œæˆæ—¥æœŸ: {clean_date_str} -> {latest_exec_date}")

                    except Exception as e:
                        logger.warning(f"âš ï¸ è½¬æ¢æœ€æ–°å®Œæˆæ—¥æœŸå¤±è´¥: {e}ï¼ŒåŸå§‹å€¼={latest_exec_date_field}ï¼Œæ¸…ç†å={clean_date_str}")
                        latest_exec_date = ""
                else:
                    latest_exec_date = ""

                # è§£æå¤šæ¨é€æ—¶é—´ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
                push_time_raw = safe_strip(fields.get("æ¨é€æ—¶é—´", ""))
                push_times = []  # å­˜å‚¨è§£æåçš„æœ‰æ•ˆæ—¶é—´åˆ—è¡¨[(hour, minute), ...]
                if push_time_raw:
                    time_str_list = [t.strip() for t in push_time_raw.split(',') if t.strip()]
                    for time_str in time_str_list:
                        time_str = time_str.replace("ï¼š", ":")
                        if re.match(r'^\d{1,2}:\d{2}$', time_str):
                            hour, minute = map(int, time_str.split(":"))
                            if 0 <= hour <= 23 and 0 <= minute <= 59:
                                push_times.append((hour, minute))
                                logger.info(f"âœ… è§£ææœ‰æ•ˆæ¨é€æ—¶é—´ï¼š{time_str}ï¼ˆä»»åŠ¡{idx}ï¼‰")
                            else:
                                logger.warning(f"âš ï¸ æ¨é€æ—¶é—´{time_str}è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡ï¼ˆä»»åŠ¡{idx}ï¼‰")
                        else:
                            logger.warning(f"âš ï¸ æ¨é€æ—¶é—´{time_str}æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡ï¼ˆä»»åŠ¡{idx}ï¼‰")

                # æ„å»ºä»»åŠ¡å­—å…¸
                task = {
                    "record_id": record_id,
                    "is_enabled": is_enabled,
                    "cron_type": cron_type,
                    "cron_extra": safe_strip(fields.get("æ¯å‘¨-å‘¨å‡ ", "")) or
                                  safe_strip(fields.get("æ¯æœˆ-å‡ å·", "")),
                    "push_times": push_times,  # å¤šæ¨é€æ—¶é—´åˆ—è¡¨
                    "push_time_raw": push_time_raw,  # åŸå§‹å­—ç¬¦ä¸²
                    "table_url": table_url,
                    "target_field": safe_strip(fields.get("å­—æ®µ", "")),
                    "webhook_title": webhook_title,
                    "webhook_url": webhook_url,
                    "at_field_name": at_field_name,
                    "latest_exec_date": latest_exec_date
                }

                # è¿‡æ»¤æ— æ•ˆä»»åŠ¡
                missing_fields = []
                if not task["is_enabled"].startswith("å·²å¯ç”¨"):
                    missing_fields.append(f"æ˜¯å¦å¯ç”¨ï¼ˆå½“å‰ï¼š{task['is_enabled']}ï¼‰")
                if not task["cron_type"]:
                    missing_fields.append("å®šæ—¶ç±»å‹")
                if not task["push_times"]:
                    missing_fields.append(f"æ¨é€æ—¶é—´ï¼ˆåŸå§‹å€¼ï¼š{task['push_time_raw']}ï¼Œæ— æœ‰æ•ˆæ—¶é—´ï¼‰")
                if not task["table_url"]:
                    missing_fields.append("è¡¨é“¾æ¥")
                if not task["target_field"]:
                    missing_fields.append("å­—æ®µ")
                if not task["webhook_url"]:
                    missing_fields.append("webhook_url")
                if task["cron_type"] == "æ¯å‘¨" and not task["cron_extra"]:
                    missing_fields.append("æ¯å‘¨-å‘¨å‡ ")
                if task["cron_type"] == "æ¯æœˆ" and not task["cron_extra"]:
                    missing_fields.append("æ¯æœˆ-å‡ å·")

                if missing_fields:
                    logger.warning(f"âš ï¸ ä»»åŠ¡{idx}é…ç½®ä¸å®Œæ•´ï¼ˆè·³è¿‡ï¼‰ï¼Œç¼ºå¤±ï¼š{','.join(missing_fields)}")
                    continue

                task_configs.append(task)
                push_times_str = ",".join([f"{h:02d}:{m:02d}" for h, m in task["push_times"]])
                logger.info(
                    f"âœ… æœ‰æ•ˆä»»åŠ¡{idx}ï¼š{task['is_enabled']} | å®šæ—¶={task['cron_type']}{task['cron_extra'] if task['cron_extra'] else ''} {push_times_str}")

            logger.info(f"ğŸ“Š å…±è·å–{len(task_configs)}ä¸ªæœ‰æ•ˆå¯ç”¨ä»»åŠ¡")
            return task_configs

        except requests.exceptions.ConnectionError as e:
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"âš ï¸ ç¬¬{retry_count}æ¬¡è¯·æ±‚å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰ï¼š{str(e)[:100]}ï¼Œ{retry_interval}ç§’åé‡è¯•")
                time.sleep(retry_interval)
            else:
                logger.error(f"âŒ ä¸‰æ¬¡è¯·æ±‚å‡å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰ï¼š{str(e)[:100]}")
                return []

        except requests.exceptions.Timeout as e:
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"âš ï¸ ç¬¬{retry_count}æ¬¡è¯·æ±‚å¤±è´¥ï¼ˆè¶…æ—¶ï¼‰ï¼š{str(e)[:100]}ï¼Œ{retry_interval}ç§’åé‡è¯•")
                time.sleep(retry_interval)
            else:
                logger.error(f"âŒ ä¸‰æ¬¡è¯·æ±‚å‡å¤±è´¥ï¼ˆè¶…æ—¶ï¼‰ï¼š{str(e)[:100]}")
                return []

        except Exception as e:
            logger.error(f"âŒ è¯»å–é…ç½®è¡¨å¤±è´¥ï¼ˆéç½‘ç»œé—®é¢˜ï¼‰ï¼š{e}")
            return []


# -------------------------- 6. å®šæ—¶è§„åˆ™è§£æï¼ˆé€‚é…å¤šæ¨é€æ—¶é—´ï¼‰ --------------------------
def parse_cron_config(task):
    """è§£æä»»åŠ¡çš„å®šæ—¶è§„åˆ™ï¼ˆæ”¯æŒå¤šæ¨é€æ—¶é—´ï¼‰"""
    cron_type = task["cron_type"]
    cron_extra = task["cron_extra"]
    push_times = task["push_times"]

    if not push_times:
        logger.warning(f"âš ï¸ ä»»åŠ¡{task['record_id']}æ— æœ‰æ•ˆæ¨é€æ—¶é—´ï¼Œè·³è¿‡è§£æ")
        return None

    try:
        if cron_type == "æ¯æ—¥":
            return {
                "type": "daily",
                "push_times": push_times,
                "desc": f"æ¯æ—¥{','.join([f'{h:02d}:{m:02d}' for h, m in push_times])}"
            }

        elif cron_type == "æ¯å‘¨":
            weekday_map = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5, "å…­": 6, "æ—¥": 0}
            if cron_extra not in weekday_map:
                logger.warning(f"âš ï¸ æ— æ•ˆå‘¨å‡ ï¼š{cron_extra}ï¼ˆä»»åŠ¡IDï¼š{task['record_id']}ï¼‰")
                return None
            return {
                "type": "weekly",
                "weekday": weekday_map[cron_extra],
                "push_times": push_times,
                "desc": f"æ¯å‘¨{cron_extra} {','.join([f'{h:02d}:{m:02d}' for h, m in push_times])}"
            }

        elif cron_type == "æ¯æœˆ":
            if not cron_extra.isdigit() or not (1 <= int(cron_extra) <= 31):
                logger.warning(f"âš ï¸ æ— æ•ˆå‡ å·ï¼š{cron_extra}ï¼ˆä»»åŠ¡IDï¼š{task['record_id']}ï¼‰")
                return None
            return {
                "type": "monthly",
                "day": int(cron_extra),
                "push_times": push_times,
                "desc": f"æ¯æœˆ{cron_extra}å· {','.join([f'{h:02d}:{m:02d}' for h, m in push_times])}"
            }

        elif cron_type == "å•æ¬¡":
            return {
                "type": "once",
                "push_times": push_times,
                "desc": f"æ¯æ—¥{','.join([f'{h:02d}:{m:02d}' for h, m in push_times])}ï¼ˆå•æ¬¡ä»»åŠ¡ï¼Œç­›é€‰æ–°å¢æ•°æ®ï¼‰"
            }

        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å®šæ—¶ç±»å‹ï¼š{cron_type}ï¼ˆä»»åŠ¡IDï¼š{task['record_id']}ï¼‰")
            return None
    except Exception as e:
        logger.error(f"âŒ è§£æå®šæ—¶è§„åˆ™å¤±è´¥ï¼š{e}ï¼ˆä»»åŠ¡IDï¼š{task['record_id']}ï¼‰")
        return None


def should_execute_now(cron_config):
    """åˆ¤æ–­å½“å‰æ—¶é—´æ˜¯å¦ç¬¦åˆå®šæ—¶è§„åˆ™ï¼ˆé€‚é…å¤šæ¨é€æ—¶é—´ï¼‰"""
    if not cron_config or "push_times" not in cron_config:
        return False
    now = datetime.now()
    current_hour = now.hour
    current_minute = now.minute
    current_weekday = now.weekday()
    current_day = now.day

    ding_weekday = current_weekday + 1 if current_weekday != 6 else 0

    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»æ„æ¨é€æ—¶é—´ç‚¹
    is_time_match = any(
        (h == current_hour and m == current_minute)
        for h, m in cron_config["push_times"]
    )
    if not is_time_match:
        return False

    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åŒ¹é…
    if cron_config["type"] == "daily":
        return True
    elif cron_config["type"] == "weekly":
        return ding_weekday == cron_config["weekday"]
    elif cron_config["type"] == "monthly":
        return current_day == cron_config["day"]
    elif cron_config["type"] == "once":
        return True
    return False


def is_missed_today_task(cron_config, latest_exec_date):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä»Šæ—¥æœªæ‰§è¡Œçš„ä»»åŠ¡ï¼ˆé€‚é…å¤šæ¨é€æ—¶é—´ï¼‰"""
    if not cron_config or not latest_exec_date or "push_times" not in cron_config:
        return False

    today = date.today()
    try:
        datetime_formats = [
            "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S",
            "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M",
            "%Y-%m-%d", "%Y/%m/%d"
        ]
        last_exec_datetime = None
        for fmt in datetime_formats:
            try:
                last_exec_datetime = datetime.strptime(latest_exec_date, fmt)
                break
            except ValueError:
                continue

        if last_exec_datetime is None and latest_exec_date.isdigit():
            timestamp_seconds = int(latest_exec_date) / 1000
            last_exec_datetime = datetime.fromtimestamp(timestamp_seconds)
        if last_exec_datetime is None:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ—¥æœŸæ ¼å¼ï¼š{latest_exec_date}")

        last_exec_date = last_exec_datetime.date()
        if last_exec_date >= today:
            return False

    except Exception as e:
        logger.warning(f"âš ï¸ è§£ææœ€æ–°æ‰§è¡Œæ—¥æœŸå¤±è´¥ï¼š{e}ï¼Œæ—¥æœŸå€¼ï¼š{latest_exec_date}")
        return False

    now = datetime.now()
    current_hour = now.hour
    current_minute = now.minute
    current_weekday = now.weekday()
    ding_weekday = current_weekday + 1 if current_weekday != 6 else 0

    # åˆ¤æ–­æ—¥æœŸæ˜¯å¦åŒ¹é…
    is_date_match = False
    if cron_config["type"] in ["daily", "once"]:
        is_date_match = True
    elif cron_config["type"] == "weekly":
        is_date_match = (ding_weekday == cron_config["weekday"])
    elif cron_config["type"] == "monthly":
        is_date_match = (today.day == cron_config["day"])

    if not is_date_match:
        return False

    # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ‰§è¡Œçš„æ—¶é—´ç‚¹
    for h, m in cron_config["push_times"]:
        if (current_hour > h) or (current_hour == h and current_minute > m):
            target_datetime = datetime.combine(today, datetime.min.time()).replace(hour=h, minute=m)
            if target_datetime > last_exec_datetime:
                return True

    return False


# -------------------------- 7. æ•°æ®æå– --------------------------
def get_table_records(table_url, target_field, cron_type="", latest_exec_date="", at_field_name=""):
    """ä»ç›®æ ‡è¡¨æå–è®°å½•ï¼Œå•æ¬¡ä»»åŠ¡ç­›é€‰æ–°å¢æ•°æ®"""
    logger.info(f"ğŸ” æå–è¡¨æ•°æ®ï¼šé“¾æ¥={table_url}ï¼Œå­—æ®µ={target_field}ï¼Œç±»å‹={cron_type}")

    table_info = parse_dingtalk_table_url(table_url)
    if not table_info:
        logger.error(f"âŒ æ— æ³•è§£æè¡¨é“¾æ¥ï¼š{table_url}")
        return []
    base_id = table_info["base_id"]
    sheet_id = table_info["sheet_id"]

    access_token = get_dingtalk_access_token()
    if not access_token:
        logger.error("âŒ æ— Tokenï¼Œæ— æ³•è¯»å–è¡¨æ•°æ®")
        return []

    # åˆ†é¡µè¯»å–æ‰€æœ‰è®°å½•
    all_records = []
    next_token = None
    page_count = 0

    while True:
        page_count += 1
        logger.info(f"ğŸ“„ è¯»å–ç¬¬{page_count}é¡µæ•°æ®...")

        url = f"https://api.dingtalk.com/v1.0/notable/bases/{base_id}/sheets/{sheet_id}/records"
        headers = {
            "x-acs-dingtalk-access-token": access_token,
            "Content-Type": "application/json"
        }

        params = {
            "maxResults": 100,
            "operatorId": DINGTALK_CONFIG["operator_id"]
        }

        if next_token:
            params["nextToken"] = next_token

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            records = data.get("records", [])
            all_records.extend(records)

            logger.info(f"âœ… ç¬¬{page_count}é¡µè·å–{len(records)}æ¡è®°å½•")

            next_token = data.get("nextToken")
            if not next_token:
                logger.info(f"ğŸ“Š æ‰€æœ‰æ•°æ®è¯»å–å®Œæˆï¼Œå…±{len(all_records)}æ¡è®°å½•")
                break

        except Exception as e:
            logger.error(f"âŒ ç¬¬{page_count}é¡µè¯»å–å¤±è´¥ï¼š{e}")
            break

    # å•æ¬¡ä»»åŠ¡ç­›é€‰æ–°å¢æ•°æ®
    filtered_records = all_records
    if cron_type == "once":
        filtered_records = []
        if not latest_exec_date:
            base_datetime = datetime.now() - timedelta(days=7)
            logger.info(f"âš ï¸ å•æ¬¡ä»»åŠ¡æ— æœ€æ–°å®Œæˆæ—¥æœŸï¼Œé»˜è®¤ç­›é€‰7å¤©å†…æ•°æ®ï¼ˆåŸºå‡†æ—¶é—´ï¼š{base_datetime.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰")
        else:
            try:
                datetime_formats = [
                    "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S",
                    "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M",
                    "%Y-%m-%d", "%Y/%m/%d"
                ]
                base_datetime = None
                for fmt in datetime_formats:
                    try:
                        base_datetime = datetime.strptime(latest_exec_date, fmt)
                        break
                    except ValueError:
                        continue
                if base_datetime is None and latest_exec_date.isdigit():
                    timestamp_seconds = int(latest_exec_date) / 1000
                    base_datetime = datetime.fromtimestamp(timestamp_seconds)
                if base_datetime is None:
                    raise ValueError(f"æ— æ³•è§£ææœ€æ–°å®Œæˆæ—¥æœŸï¼š{latest_exec_date}")
                logger.info(f"ğŸ” å•æ¬¡ä»»åŠ¡ç­›é€‰åŸºå‡†ï¼šåˆ›å»ºæ—¶é—´ > {base_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            except Exception as e:
                logger.warning(f"âš ï¸ è§£ææœ€æ–°å®Œæˆæ—¥æœŸå¤±è´¥ï¼Œé»˜è®¤ç­›é€‰7å¤©å†…æ•°æ®ï¼š{e}")
                base_datetime = datetime.now() - timedelta(days=7)

        # ç­›é€‰é€»è¾‘ï¼šåˆ›å»ºæ—¶é—´ > åŸºå‡†æ—¶é—´
        datetime_formats = [
            "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S",
            "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M",
            "%Y-%m-%d", "%Y/%m/%d"
        ]

        for record in all_records:
            fields = record.get("fields", {})
            record_id = safe_strip(record.get("recordId") or record.get("id"))
            create_time_value = safe_strip(fields.get("åˆ›å»ºæ—¶é—´", ""))

            if not create_time_value:
                logger.debug(f"âš ï¸ è®°å½•{record_id}æ— ã€Œåˆ›å»ºæ—¶é—´ã€å­—æ®µï¼Œè·³è¿‡")
                continue

            try:
                create_time = None
                for fmt in datetime_formats:
                    try:
                        create_time = datetime.strptime(create_time_value, fmt)
                        break
                    except ValueError:
                        continue

                if create_time is None and create_time_value.isdigit():
                    timestamp_seconds = int(create_time_value) / 1000
                    create_time = datetime.fromtimestamp(timestamp_seconds)
                if create_time is None:
                    raise ValueError(f"æ— æ³•è¯†åˆ«çš„åˆ›å»ºæ—¶é—´æ ¼å¼ï¼š{create_time_value}")

                if create_time > base_datetime:
                    filtered_records.append(record)
                    logger.info(f"âœ… è®°å½•{record_id}ç¬¦åˆæ¡ä»¶ï¼šåˆ›å»ºæ—¶é—´={create_time.strftime('%Y-%m-%d %H:%M:%S')} > åŸºå‡†æ—¶é—´")
                else:
                    logger.debug(f"âŒ è®°å½•{record_id}ä¸ç¬¦åˆæ¡ä»¶ï¼šåˆ›å»ºæ—¶é—´â‰¤åŸºå‡†æ—¶é—´")

            except Exception as e:
                logger.warning(f"âš ï¸ è§£æè®°å½•{record_id}çš„ã€Œåˆ›å»ºæ—¶é—´ã€å¤±è´¥ï¼š{e}ï¼Œå€¼={create_time_value}")
                continue

        logger.info(f"ğŸ” å•æ¬¡ä»»åŠ¡ç­›é€‰å®Œæˆï¼šåŸ{len(all_records)}æ¡ â†’ ä¿ç•™{len(filtered_records)}æ¡æ–°å¢è®°å½•")

    # æå–å†…å®¹å’Œ@äººä¿¡æ¯
    content_list = []
    for record in filtered_records:
        fields = record.get("fields", {})
        record_id = safe_strip(record.get("recordId") or record.get("id"))

        target_value = fields.get(target_field, "")
        if isinstance(target_value, dict):
            content = safe_strip(target_value.get("text", "") or target_value.get("name", ""))
        else:
            content = safe_strip(target_value)

        user_id = ""
        if at_field_name:
            at_value = safe_strip(fields.get(at_field_name, ""))
            logger.info(f"ğŸ“Œ è®°å½•{record_id}ï¼š@äººå­—æ®µåç§°={at_field_name}ï¼ŒåŸå§‹å€¼={at_value}")
            user_id = at_value

        if content:
            content_list.append({"content": content, "user_id": user_id})
            logger.info(f"ğŸ“ è®°å½•{record_id}ï¼šcontent={content[:50]}...ï¼Œuser_id={'[ç©º]' if not user_id else user_id}")

    return content_list


# -------------------------- 8. Webhookå‘é€ --------------------------
def send_webhook(task, content, user_id=""):
    """å‘é€Webhookæ¶ˆæ¯ï¼Œæ”¯æŒ@æŒ‡å®šç”¨æˆ·"""
    if not task["webhook_url"]:
        logger.error("âŒ Webhook URL ä¸ºç©ºï¼Œæ— æ³•å‘é€")
        return False

    title = task["webhook_title"] or ""
    body = content

    if not title:
        title_match = re.search(r'^#\s*(.+?)(?:\n|$)', content)
        if title_match:
            title = title_match.group(1).strip()
            logger.info(f"âœ… ä»å†…å®¹ä¸­æå–åˆ°æ ‡é¢˜: '{title}'")
        else:
            title = content[:30].strip() + "..." if len(content) > 30 else content
            logger.info(f"â„¹ï¸ æœªæ‰¾åˆ°æ ‡é¢˜æ ¼å¼ï¼Œä½¿ç”¨å†…å®¹å‰éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜: '{title}'")

    markdown_text = f"{body}\n\n> æœ¬æ¶ˆæ¯ç”±å®šæ—¶ä»»åŠ¡è‡ªåŠ¨å‘é€"
    if user_id.strip():
        markdown_text += f"\n\n@{user_id}"

    message = {
        "msgtype": "markdown",
        "markdown": {
            "title": title,
            "text": markdown_text
        },
        "at": {
            "atUserIds": [user_id.strip()] if user_id.strip() else [],
            "isAtAll": False
        }
    }

    message_log = json.dumps(message, ensure_ascii=False, indent=2).replace(task["webhook_url"], "***")
    logger.info(f"ğŸ“¤ æœ€ç»ˆå‘é€æ¶ˆæ¯ä½“ï¼š{message_log}")

    try:
        logger.info(f"ğŸ“¤ å‘é€ Markdown æ¶ˆæ¯ï¼šæ ‡é¢˜='{title}'ï¼Œå†…å®¹é•¿åº¦={len(body)}ï¼Œ{'åŒ…å«è‰¾ç‰¹' if user_id.strip() else 'ä¸åŒ…å«è‰¾ç‰¹'}")
        resp = requests.post(
            task["webhook_url"],
            headers={"Content-Type": "application/json; charset=utf-8"},
            data=json.dumps(message, ensure_ascii=False).encode("utf-8"),
            timeout=10
        )
        result = resp.json()
        if result.get("errcode") == 0:
            logger.info("âœ… Webhook å‘é€æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ Webhook å‘é€å¤±è´¥ï¼š{result.get('errmsg')}")
            return False
    except Exception as e:
        logger.error(f"âŒ Webhook å‘é€å¼‚å¸¸ï¼š{e}")
        return False


# -------------------------- 9. æ›´æ–°æ‰§è¡Œæ—¥æœŸ --------------------------
def update_task_exec_date(record_id):
    """æ›´æ–°ä»»åŠ¡çš„æœ€æ–°æ‰§è¡Œæ—¥æœŸ"""
    if not record_id:
        logger.warning("âš ï¸ è®°å½•IDä¸ºç©ºï¼Œæ— æ³•æ›´æ–°æ‰§è¡Œæ—¥æœŸ")
        return False

    access_token = get_dingtalk_access_token()
    if not access_token:
        logger.error("âŒ æ— Tokenï¼Œæ— æ³•æ›´æ–°æ‰§è¡Œæ—¥æœŸ")
        return False

    base_id = DINGTALK_CONFIG["config_base_id"]
    sheet_name = urllib.parse.quote(DINGTALK_CONFIG["config_sheet_name"])
    url = f"https://api.dingtalk.com/v1.0/notable/bases/{base_id}/sheets/{sheet_name}/records"
    headers = {
        "x-acs-dingtalk-access-token": access_token,
        "Content-Type": "application/json"
    }

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    payload = {
        "records": [
            {
                "id": record_id,
                "fields": {"æœ€æ–°å®Œæˆæ—¥æœŸ": current_time}
            }
        ],
        "operatorId": DINGTALK_CONFIG["operator_id"]
    }

    try:
        response = requests.put(url, headers=headers, json=payload, timeout=15)
        response.raise_for_status()
        logger.info(f"âœ… å·²æ›´æ–°ä»»åŠ¡{record_id}çš„æœ€æ–°æ‰§è¡Œæ—¥æœŸä¸ºï¼š{current_time}")
        return True
    except requests.exceptions.HTTPError as e:
        response = e.response
        logger.error(f"âŒ æ›´æ–°ä»»åŠ¡{record_id}æ‰§è¡Œæ—¥æœŸå¤±è´¥ï¼ˆHTTP {response.status_code}ï¼‰ï¼š")
        logger.error(f"å“åº”å†…å®¹ï¼š{response.text}")
        return False
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä»»åŠ¡{record_id}æ‰§è¡Œæ—¥æœŸå¤±è´¥ï¼š{e}")
        return False


# -------------------------- 10. ä»»åŠ¡æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ --------------------------
def check_and_execute_tasks():
    """æ£€æŸ¥å¹¶æ‰§è¡Œæ‰€æœ‰å¯ç”¨çš„ä»»åŠ¡"""
    logger.info("=" * 60)
    logger.info("ğŸ” å¼€å§‹æ£€æŸ¥å¾…æ‰§è¡Œä»»åŠ¡...")
    now = datetime.now()
    today = date.today().strftime("%Y-%m-%d")
    current_time_str = now.strftime("%H:%M")
    weekday_names = ["ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "æ—¥"]
    current_weekday = weekday_names[now.weekday()]
    logger.info(f"â° å½“å‰æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆå‘¨{current_weekday}ï¼Œå½“å‰æ—¶åˆ»ï¼š{current_time_str}ï¼‰")

    tasks = get_task_configs()
    if not tasks:
        logger.warning("âš ï¸ æ— æœ‰æ•ˆä»»åŠ¡ï¼Œç»“æŸæ£€æŸ¥")
        return

    normal_execution_count = 0
    compensation_execution_count = 0

    for idx, task in enumerate(tasks, 1):
        push_times_str = ",".join([f"{h:02d}:{m:02d}" for h, m in task["push_times"]])
        logger.info(
            f"\nğŸ“‹ å¤„ç†ä»»åŠ¡{idx}ï¼š{task['is_enabled']} | å®šæ—¶è§„åˆ™={task['cron_type']}{task['cron_extra'] if task['cron_extra'] else ''} | æ¨é€æ—¶é—´={push_times_str}")

        cron_config = parse_cron_config(task)
        if not cron_config:
            logger.warning(f"âš ï¸ ä»»åŠ¡{idx}å®šæ—¶è§„åˆ™æ— æ•ˆï¼Œè·³è¿‡")
            continue

        # æ­£å¸¸æ‰§è¡Œ
        if should_execute_now(cron_config):
            matched_time = \
            [f"{h:02d}:{m:02d}" for h, m in cron_config["push_times"] if h == now.hour and m == now.minute][0]
            logger.info(f"ğŸ¯ ä»»åŠ¡{idx}è§¦å‘æ­£å¸¸æ‰§è¡Œï¼ˆå½“å‰åŒ¹é…æ—¶é—´ç‚¹ï¼š{matched_time}ï¼‰...")

            content_list = get_table_records(
                table_url=task["table_url"],
                target_field=task["target_field"],
                cron_type=cron_config["type"],
                latest_exec_date=task["latest_exec_date"],
                at_field_name=task["at_field_name"]
            )

            if not content_list:
                logger.info(f"â„¹ï¸ ä»»åŠ¡{idx}æ— ç¬¦åˆæ¡ä»¶çš„è®°å½•ï¼Œæ— éœ€å‘é€")
                update_task_exec_date(task["record_id"])
                normal_execution_count += 1
                continue

            send_success = True
            for i, item in enumerate(content_list, 1):
                logger.info(f"ğŸ“¤ ä»»åŠ¡{idx}ï¼ˆ{matched_time}ï¼‰å‘é€ç¬¬{i}/{len(content_list)}æ¡è®°å½•")
                if not send_webhook(task, item["content"], item["user_id"]):
                    logger.error(f"ğŸ’¥ ä»»åŠ¡{idx}ï¼ˆ{matched_time}ï¼‰ç¬¬{i}æ¡è®°å½•å‘é€å¤±è´¥")
                    send_success = False
                else:
                    logger.info(f"âœ… ä»»åŠ¡{idx}ï¼ˆ{matched_time}ï¼‰ç¬¬{i}æ¡è®°å½•å‘é€æˆåŠŸ")

            update_task_exec_date(task["record_id"])
            if send_success:
                logger.info(f"ğŸ‰ ä»»åŠ¡{idx}ï¼ˆ{matched_time}ï¼‰æ‰€æœ‰è®°å½•å‘é€å®Œæˆ")
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡{idx}ï¼ˆ{matched_time}ï¼‰éƒ¨åˆ†è®°å½•å‘é€å¤±è´¥")
            normal_execution_count += 1

        # è¡¥å¿æ‰§è¡Œ
        elif is_missed_today_task(cron_config, task.get("latest_exec_date", "")):
            missed_times = [
                f"{h:02d}:{m:02d}" for h, m in cron_config["push_times"]
                if (now.hour > h) or (now.hour == h and now.minute > m)
            ]
            logger.info(f"â³ ä»»åŠ¡{idx}è§¦å‘è¡¥å¿æ‰§è¡Œï¼ˆæœªæ‰§è¡Œçš„æ—¶é—´ç‚¹ï¼š{','.join(missed_times)}ï¼‰...")

            content_list = get_table_records(
                table_url=task["table_url"],
                target_field=task["target_field"],
                cron_type=cron_config["type"],
                latest_exec_date=task["latest_exec_date"],
                at_field_name=task["at_field_name"]
            )

            if not content_list:
                logger.info(f"â„¹ï¸ ä»»åŠ¡{idx}è¡¥å¿æ‰§è¡Œæ— ç¬¦åˆæ¡ä»¶çš„è®°å½•")
                update_task_exec_date(task["record_id"])
                compensation_execution_count += 1
                continue

            send_success = True
            for i, item in enumerate(content_list, 1):
                logger.info(f"ğŸ“¤ ä»»åŠ¡{idx}ï¼ˆè¡¥å¿ï¼‰å‘é€ç¬¬{i}/{len(content_list)}æ¡è®°å½•")
                if not send_webhook(task, item["content"], item["user_id"]):
                    logger.error(f"ğŸ’¥ ä»»åŠ¡{idx}ï¼ˆè¡¥å¿ï¼‰ç¬¬{i}æ¡è®°å½•å‘é€å¤±è´¥")
                    send_success = False
                else:
                    logger.info(f"âœ… ä»»åŠ¡{idx}ï¼ˆè¡¥å¿ï¼‰ç¬¬{i}æ¡è®°å½•å‘é€æˆåŠŸ")

            update_task_exec_date(task["record_id"])
            if send_success:
                logger.info(f"ğŸ‰ ä»»åŠ¡{idx}è¡¥å¿æ‰§è¡Œå®Œæˆï¼ˆå·²è¡¥å…¨{','.join(missed_times)}çš„æ‰§è¡Œï¼‰")
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡{idx}è¡¥å¿æ‰§è¡Œéƒ¨åˆ†è®°å½•å¤±è´¥")
            compensation_execution_count += 1

        else:
            logger.debug(f"â³ ä»»åŠ¡{idx}æœªåˆ°æ‰§è¡Œæ—¶é—´ï¼ˆå½“å‰æ—¶åˆ»{current_time_str}ï¼ŒæœªåŒ¹é…ä»»ä½•æ¨é€æ—¶é—´ç‚¹ï¼‰")

    logger.info(
        f"\nğŸ“Š æœ¬æ¬¡ä»»åŠ¡æ£€æŸ¥å®Œæˆï¼šæ­£å¸¸æ‰§è¡Œ{normal_execution_count}ä¸ªï¼Œè¡¥å¿æ‰§è¡Œ{compensation_execution_count}ä¸ªï¼Œæ€»è®¡{normal_execution_count + compensation_execution_count}ä¸ª")
    logger.info("=" * 60)


# -------------------------- 11. ä¸»å‡½æ•° --------------------------
def main():
    logger.info("ğŸš€ æ‹‰ä¼¸å¤§å¸ˆå®šæ—¶Webhookè°ƒåº¦å™¨å¯åŠ¨")
    logger.info("ğŸ“– æ”¯æŒçš„å®šæ—¶ç±»å‹ï¼ˆæ¨é€æ—¶é—´æ”¯æŒå¤šå€¼ï¼Œå¦‚8:00,10:00ï¼‰ï¼š")
    logger.info("   - æ¯æ—¥ï¼šå®šæ—¶ç±»å‹=æ¯æ—¥ï¼Œæ¨é€æ—¶é—´=HH:MM,HH:MM â†’ æ¯å¤©å¤šæ—¶é—´ç‚¹æ‰§è¡Œï¼Œå–å…¨éƒ¨æ•°æ®")
    logger.info("   - æ¯å‘¨ï¼šå®šæ—¶ç±»å‹=æ¯å‘¨ï¼Œæ¯å‘¨-å‘¨å‡ =ä¸€~æ—¥ï¼Œæ¨é€æ—¶é—´=HH:MM,HH:MM â†’ æ¯å‘¨æŒ‡å®šæ—¥å¤šæ—¶é—´ç‚¹æ‰§è¡Œ")
    logger.info("   - æ¯æœˆï¼šå®šæ—¶ç±»å‹=æ¯æœˆï¼Œæ¯æœˆ-å‡ å·=1~31ï¼Œæ¨é€æ—¶é—´=HH:MM,HH:MM â†’ æ¯æœˆæŒ‡å®šæ—¥å¤šæ—¶é—´ç‚¹æ‰§è¡Œ")
    logger.info("   - å•æ¬¡ï¼šå®šæ—¶ç±»å‹=å•æ¬¡ï¼Œæ¨é€æ—¶é—´=HH:MM,HH:MM â†’ æ¯å¤©å¤šæ—¶é—´ç‚¹æ‰§è¡Œï¼Œç­›é€‰æ–°å¢æ•°æ®")
    logger.info("ğŸ’¡ æç¤ºï¼š@äººå­—æ®µä¸ºç©ºæ—¶ä¸æ·»åŠ è‰¾ç‰¹ï¼Œæ— æ•ˆæ¨é€æ—¶é—´ä¼šè‡ªåŠ¨è¿‡æ»¤")
    logger.info("ğŸ’¡ æŒ‰Ctrl+Cåœæ­¢è°ƒåº¦å™¨")
    logger.info("=" * 60)

    # å¯åŠ¨æ—¶å…ˆæ£€æŸ¥ä¸€æ¬¡ä»»åŠ¡
    check_and_execute_tasks()

    # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿å¤šæ—¶é—´ç‚¹åŠæ—¶è§¦å‘
    schedule.every(1).minutes.do(check_and_execute_tasks)
    logger.info("ğŸ‘‚ å¼€å§‹ç›‘å¬ä»»åŠ¡ï¼ˆæ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼‰...")

    try:
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­è°ƒåº¦å™¨...")
    except Exception as e:
        logger.error(f"ğŸ’¥ è°ƒåº¦å™¨è¿è¡Œå¼‚å¸¸ï¼š{e}")
    finally:
        logger.info("ğŸ‘‹ è°ƒåº¦å™¨å·²åœæ­¢")


if __name__ == "__main__":
    main()
