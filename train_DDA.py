import timeit
import argparse
import numpy as np
import pandas as pd
import torch.optim as optim
import torch
import torch.nn as nn
import torch.nn.functional as fn
from data_preprocess import *
from model.AMNTDDA import AMNTDDA
from metric import *
import scipy.sparse as sp
from dgl import DGLGraph
from collections import defaultdict
from scipy.sparse import csr_matrix
from modeling.saits import SAITS
from modeling.loss_functions import mit_loss, ort_loss
from torch.utils.data import DataLoader, TensorDataset
from modeling.utils import masked_mae_cal
from hypergraph_utils import construct_hypergraph,Multi_omics_hyperedge_concat,construct_H_with_KNN_from_distance,generate_G_from_H
import sklearn.metrics as metrics
from datetime import datetime
import sys  # <--- å¿…é¡»æœ‰è¿™ä¸€è¡Œ
# from DualHyperGT import DualHyperGT

device = torch.device('cuda')

# === æ–°å¢ï¼šæ—¥å¿—è®°å½•ç±» ===
class Logger(object):
    def __init__(self, filename="Default.log"):
        self.terminal = sys.stdout  # å±å¹•è¾“å‡ºæµ
        self.log = open(filename, "a", encoding='utf-8')  # æ–‡ä»¶è¾“å‡ºæµ
    
    def write(self, message):
        self.terminal.write(message)  # æ‰“å°åˆ°å±å¹•
        self.log.write(message)       # å†™å…¥åˆ°æ–‡ä»¶
        self.log.flush()              # ç«‹å³åˆ·æ–°ç¼“å†²åŒºï¼Œé˜²æ­¢ç¨‹åºå´©æºƒä¸¢å¤±æ—¥å¿—
    
    def flush(self):
        pass


def train(args, model, train_loader, optimizer, epoch,
          drdr_graph, didi_graph, drdipr_graph,
          drug_feature, disease_feature, protein_feature,
          H_drdr, H_didi):
    model.train()
    #total_cls_loss = 0.0  # åˆ†ç±»æŸå¤±ï¼ˆä¸»ä»»åŠ¡ï¼‰
    total_drug_reg_loss = 0.0  # è¯ç‰©SAITSå›å½’æŸå¤±ï¼ˆè¾…åŠ©ï¼‰
    total_disease_reg_loss = 0.0  # ç–¾ç—…SAITSå›å½’æŸå¤±ï¼ˆè¾…åŠ©ï¼‰
    #total_acc = 0.0  # åˆ†ç±»å‡†ç¡®ç‡ï¼ˆè¾…åŠ©ç›‘æ§ï¼‰

    # åˆ†ç±»æŸå¤±å‡½æ•°ï¼ˆä¸»ä»»åŠ¡ï¼‰
    cls_criterion = nn.CrossEntropyLoss()

    for batch_idx, batch in enumerate(train_loader):
        # è§£æbatchæ•°æ®ï¼šæ ·æœ¬ç´¢å¼•ã€åˆ†ç±»æ ‡ç­¾
        sample = batch[0].to(device)  # æ ·æœ¬ç´¢å¼• (batch_size, 2)
        y_cls = batch[1].to(device).squeeze()  # å‹ç¼©ç»´åº¦

        # å‰å‘ä¼ æ’­ï¼šè·å–è¾…åŠ©å›å½’æŸå¤±å’Œåˆ†ç±»è¾“å‡º
        drug_reg_loss, disease_reg_loss, cls_output = model(
            drdr_graph, didi_graph, drdipr_graph,
            drug_feature, disease_feature, protein_feature,
            sample, H_drdr, H_didi
        )

        # è®¡ç®—ä¸»ä»»åŠ¡åˆ†ç±»æŸå¤±
        cls_loss = cls_criterion(cls_output, y_cls)

        # è”åˆä¼˜åŒ–æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è¾…åŠ©å›å½’æŸå¤±
        total_loss = cls_loss + drug_reg_loss + disease_reg_loss
        # total_loss = drug_reg_loss + disease_reg_loss

        # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # ç´¯è®¡æŸå¤±
        #total_cls_loss += cls_loss.item()
        total_drug_reg_loss += drug_reg_loss.item()
        total_disease_reg_loss += disease_reg_loss.item()

        # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼ˆä»…ç”¨äºç›‘æ§ï¼‰
        #pred_cls = torch.argmax(cls_output, dim=-1)  # é¢„æµ‹ç±»åˆ«
        #acc = (pred_cls == y_cls).float().mean()
        #total_acc += acc.item()

        # æ‰“å°æ‰¹æ¬¡ä¿¡æ¯
        if (batch_idx + 1) % 10 == 0:
            print(f"Train Epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}, "
                 # f"åˆ†ç±»æŸå¤±: {cls_loss.item():.4f}, "
                  f"è¯ç‰©å›å½’æŸå¤±: {drug_reg_loss.item():.4f}, "
                  f"ç–¾ç—…å›å½’æŸå¤±: {disease_reg_loss.item():.4f}, ")
                  #f"å‡†ç¡®ç‡: {acc.item():.4f}")

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    #avg_cls_loss = total_cls_loss / len(train_loader)
    avg_drug_reg_loss = total_drug_reg_loss / len(train_loader)
    avg_disease_reg_loss = total_disease_reg_loss / len(train_loader)
    #avg_acc = total_acc / len(train_loader)
    
    avg_total_loss = avg_drug_reg_loss + avg_disease_reg_loss

    print(f"\næœ¬è½®è®­ç»ƒç»“æŸï¼š")
    print(#f"å¹³å‡åˆ†ç±»æŸå¤±: {avg_cls_loss:.4f}, "
          f"å¹³å‡è¯ç‰©å›å½’æŸå¤±: {avg_drug_reg_loss:.4f}, "
          f"å¹³å‡ç–¾ç—…å›å½’æŸå¤±: {avg_disease_reg_loss:.4f}, ")
          #f"å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")

    return avg_total_loss, 0.0#, avg_cls_loss  # è¿”å›æ€»æŸå¤±å’Œä¸»ä»»åŠ¡åˆ†ç±»æŸå¤±ï¼ˆç”¨äºæ—©åœï¼‰


def validate(args, model, val_loader,
             drdr_graph, didi_graph, drdipr_graph,
             drug_feature, disease_feature, protein_feature,
             H_drdr, H_didi):
    model.eval()
    #total_cls_loss = 0.0
    total_drug_reg_loss = 0.0
    total_disease_reg_loss = 0.0
    #total_acc = 0.0

    # å­˜å‚¨æ‰€æœ‰é¢„æµ‹æ¦‚ç‡å’Œæ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—AUCã€AUPRï¼‰
    all_pred_probs = []
    all_labels = []
    #éªŒè¯é›†ä¹Ÿè¦è®¡ç®—åˆ†ç±»æŸå¤± 
    cls_criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for batch in val_loader:
            sample = batch[0].to(device) # æ ·æœ¬ç´¢å¼•
            y_cls = batch[1].to(device).squeeze()  # æ–°å¢ .squeeze()
            labels_np = y_cls.cpu().numpy()  # è½¬æˆnumpyå­˜çœŸå®æ ‡ç­¾
            all_labels.extend(labels_np)  # ç´¯è®¡æ‰€æœ‰çœŸå®æ ‡ç­¾

            # å‰å‘ä¼ æ’­
            drug_reg_loss, disease_reg_loss, cls_output = model(
                drdr_graph, didi_graph, drdipr_graph,
                drug_feature, disease_feature, protein_feature,
                sample, H_drdr, H_didi
            )

            # è®¡ç®—æŸå¤±
            #cls_loss = cls_criterion(cls_output, y_cls)
            #total_cls_loss += cls_loss.item()
            total_drug_reg_loss += drug_reg_loss.item()
            total_disease_reg_loss += disease_reg_loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            #pred_cls = torch.argmax(cls_output, dim=-1)
            #acc = (pred_cls == y_cls).float().mean()
            #total_acc += acc.item()

            # ä¿å­˜æ­£ç±»é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºAUCè®¡ç®—ï¼‰
            #pred_probs = torch.softmax(cls_output, dim=-1)[:, 1].cpu().numpy()
            #all_pred_probs.extend(pred_probs)


    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    #avg_cls_loss = total_cls_loss / len(val_loader)
    avg_drug_reg_loss = total_drug_reg_loss / len(val_loader)
    avg_disease_reg_loss = total_disease_reg_loss / len(val_loader)
    #avg_acc = total_acc / len(val_loader)

    # 1. æ ¹æ®é¢„æµ‹æ¦‚ç‡ç”Ÿæˆé¢„æµ‹ç±»åˆ«ï¼ˆä»¥0.5ä¸ºé˜ˆå€¼ï¼Œæ¦‚ç‡>=0.5è§†ä¸ºæ­£ç±»ï¼‰
    #pred_classes = (np.array(all_pred_probs) >= 0.5).astype(int)  # è½¬æ¢ä¸º0/1çš„æ•´æ•°ç±»åˆ«
    # 2. è°ƒç”¨get_metricï¼Œä¼ å…¥ä¸‰ä¸ªå‚æ•°ï¼ˆä¸å‡½æ•°å®šä¹‰ä¸€è‡´ï¼‰
   # AUC, AUPR, accuracy, precision, recall, f1, mcc = get_metric(
   #     all_labels,  # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šçœŸå®æ ‡ç­¾ï¼ˆy_trueï¼‰
   #    pred_classes,  # ç¬¬äºŒä¸ªå‚æ•°ï¼šé¢„æµ‹ç±»åˆ«ï¼ˆy_predï¼‰
   #     all_pred_probs  # ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šé¢„æµ‹æ¦‚ç‡ï¼ˆy_probï¼‰
#)

    # æ‰“å°éªŒè¯é›†ç»“æœ
    print(f"\néªŒè¯é›†ï¼š")
    print(#f"å¹³å‡åˆ†ç±»æŸå¤±: {avg_cls_loss:.4f}, "
          f"å¹³å‡è¯ç‰©å›å½’æŸå¤±: {avg_drug_reg_loss:.4f}, "
          f"å¹³å‡ç–¾ç—…å›å½’æŸå¤±: {avg_disease_reg_loss:.4f}, ")
          #f"å‡†ç¡®ç‡: {avg_acc:.4f}")
    #print(f"AUC: {AUC:.4f}, AUPR: {AUPR:.4f}, "
    #      f"Precision: {precision:.4f}, Recall: {recall:.4f}, "
    #      f"F1-score: {f1:.4f}, MCC: {mcc:.4f}")

    total_val_loss = avg_drug_reg_loss + avg_disease_reg_loss
    return total_val_loss, 0.0#, avg_cls_loss  # è¿”å›loss, mae


#è¶…å›¾æ„å»ºæ¨¡å—
# ä¼ å…¥å›¾å¯¹è±¡,æ ¹æ®æˆå¯¹è¾¹å’Œké˜¶é‚»å±…ç”Ÿæˆè¶…å›¾,è¿”å›è¶…å›¾çš„èŠ‚ç‚¹é›†åˆå’Œè¶…è¾¹é›†åˆ
# def generate_hypergraph_matrix(graph, k=1):
#     """
#     æ ¹æ®æˆå¯¹è¾¹å’Œké˜¶é‚»å±…ç”Ÿæˆè¶…å›¾ã€‚

#     :param graph: DGLGraph å¯¹è±¡
#     :param k: é‚»å±…é˜¶æ•°,é»˜è®¤ä¸º1
#     :return: è¶…å›¾çš„èŠ‚ç‚¹é›†åˆå’Œè¶…è¾¹é›†åˆ
#     """
#     """
#     - è¾“å…¥ï¼šæ™®é€šå›¾ (node1-node2 è¿™æ ·çš„æˆå¯¹è¾¹)
#     - è¾“å‡ºï¼šè¶…å›¾çŸ©é˜µ H (èŠ‚ç‚¹Ã—è¶…è¾¹çš„å…³è”çŸ©é˜µ)
#     - æ™®é€šå›¾çš„è¾¹ï¼šåªèƒ½è¿æ¥2ä¸ªèŠ‚ç‚¹ï¼Œå¦‚ (drug1, drug2)
#     - è¶…è¾¹ï¼šå¯ä»¥è¿æ¥å¤šä¸ªèŠ‚ç‚¹ï¼Œå¦‚ (drug1, drug2, drug3, drug4)

#     å‚æ•°:
#     - graph: DGLå›¾å¯¹è±¡ (ä¾‹å¦‚è¯ç‰©ç›¸ä¼¼æ€§å›¾drdr_graph)
#     - k: é‚»å±…é˜¶æ•° (k=1è¡¨ç¤ºåªè€ƒè™‘ç›´æ¥é‚»å±…)

#     è¿”å›:
#     - HçŸ©é˜µ: torchç¨€ç–å¼ é‡ [N_nodes Ã— N_hyperedges]
#     """
#      # === ç¬¬1æ­¥: æå–åŸºç¡€ä¿¡æ¯ ===
#     # è·å–èŠ‚ç‚¹æ•°é‡å’Œè¾¹åˆ—è¡¨
#     num_nodes = graph.number_of_nodes()
#     #æå–æ‰€æœ‰è¾¹ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨ [(node1, node2), (node3, node4),
#     #zip(*graph.edges()) æ˜¯Pythonçš„è§£åŒ…æŠ€å·§ï¼Œå°†è¾¹çš„èµ·ç‚¹å’Œç»ˆç‚¹åˆ†åˆ«è§£åŒ…æˆä¸¤ä¸ªåˆ—è¡¨
#     #graph.edges() è¿”å› ([æºèŠ‚ç‚¹åˆ—è¡¨], [ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨])
#     #zip(*...) å°†å®ƒä»¬é…å¯¹-ä¾‹å¦‚: [(drug1, drug5), (drug2, drug8), ...]
#     edges = list(zip(*graph.edges()))

#     # åˆå§‹åŒ–è¶…è¾¹é›†åˆ
#     # === ç¬¬2æ­¥: åˆå§‹åŒ–è¶…è¾¹é›†åˆ ===
#     hyperedges = set(edges)  # æˆå¯¹è¾¹ä½œä¸ºåˆå§‹è¶…è¾¹
#      # ğŸ“Œ å°†æ™®é€šè¾¹ä½œä¸ºåˆå§‹è¶…è¾¹
#     # ğŸ’¡ å…³é”®ç‚¹: æ¯æ¡äºŒå…ƒè¾¹ (u, v) ä¹Ÿæ˜¯ä¸€ä¸ªè¶…è¾¹ï¼
#     # ä¾‹å¦‚: {(0,1), (0,5), (1,3), ...}
#     # æ­¤æ—¶æ¯ä¸ªè¶…è¾¹åªåŒ…å«2ä¸ªèŠ‚ç‚¹

#     # ä½¿ç”¨ BFS æˆ– DFS æ¥æ‰¾åˆ° k é˜¶é‚»å±…
#     # === ç¬¬3æ­¥: æ„å»ºé‚»å±…å­—å…¸ ===
#     # ä½¿ç”¨ BFS æˆ– DFS æ¥æ‰¾åˆ° k é˜¶é‚»å±…
#     # === ç¬¬3æ­¥: æ„å»ºé‚»å±…å­—å…¸ ===
#     neighbors = defaultdict(set)
#     # ğŸ“Œ åˆ›å»ºå­—å…¸å­˜å‚¨æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…
#     # defaultdict(set) è¡¨ç¤ºé»˜è®¤å€¼æ˜¯ç©ºé›†åˆ
#     for u, v in edges:
#         # éå†æ¯æ¡è¾¹
#         neighbors[u].add(v) # uçš„é‚»å±…ä¸­åŠ å…¥v
#         neighbors[v].add(u) # vçš„é‚»å±…ä¸­åŠ å…¥u (æ— å‘å›¾)

#     #=== ç¬¬4æ­¥: æ‰©å±•åˆ°ké˜¶é‚»å±… (æ ¸å¿ƒ!) ===
#     for _ in range(k - 1):
#         new_neighbors = defaultdict(set)
#         for node in range(num_nodes):
#             for neighbor in neighbors[node]:
#                 new_neighbors[node] |= neighbors[neighbor]
#             new_neighbors[node] -= {node}  # å»é™¤è‡ªèº«
#         neighbors = new_neighbors

#     # å°† k é˜¶é‚»å±…æ·»åŠ åˆ°è¶…è¾¹é›†åˆä¸­
#     for node, neighbor_set in neighbors.items():
#         if len(neighbor_set) > 0:
#             hyperedges.add(tuple(sorted([node] + list(neighbor_set))))

#     H = build_hypergraph_matrix(num_nodes, hyperedges)
#     return csr_to_sparse_tensor(H)  # è¿”å›torchç¨€ç–å¼ é‡


# é€šè¿‡èŠ‚ç‚¹æ•°é‡å’Œè¶…è¾¹é›†åˆ,æ„å»ºè¶…å›¾çš„å…³è”çŸ©é˜µH,è¿”å›çš„æ˜¯å…³è”çŸ©é˜µH
def build_hypergraph_matrix(num_nodes, hyperedges):
    """
    æ„å»ºè¶…å›¾çš„å…³è”çŸ©é˜µ Hã€‚

    :param num_nodes: èŠ‚ç‚¹æ•°é‡
    :param hyperedges: è¶…è¾¹é›†åˆ
    :return: å…³è”çŸ©é˜µ H (ç¨€ç–çŸ©é˜µ)
    """
    rows = []
    cols = []
    for i, hyperedge in enumerate(hyperedges):
        for node in hyperedge:
            rows.append(node)
            cols.append(i)

    data = np.ones(len(rows))
    # æ„å»ºè¿™ç§(å‹ç¼©ç¨€ç–è¡ŒçŸ©é˜µ)ï¼Œé€šè¿‡æŒ‡å®šéé›¶å…ƒç´ çš„å€¼ã€éé›¶å…ƒç´ æ‰€åœ¨çš„è¡Œç´¢å¼•ä»¥åŠåˆ—ç´¢å¼•æ¥åˆ›å»ºä¸€ä¸ªç¨€ç–çŸ©é˜µ(data, (rows, cols))ã€‚
    H = sp.csr_matrix((data, (rows, cols)), shape=(num_nodes, len(hyperedges)))
    return H


# å°† scipy.sparse.csr_matrix è½¬æ¢ä¸º torch.sparse_coo_tensor
def csr_to_sparse_tensor(csr_matrix):
    """
    :param csr_matrix: scipy.sparse.csr_matrix å¯¹è±¡
    :return: torch.sparse_coo_tensor å¯¹è±¡
    æŠŠscipyä¸­çš„ç¨€ç–çŸ©é˜µè½¬æ¢æˆtorchä¸­çš„ç¨€ç–å¼ é‡,æå‡æ•ˆç‡
    """
    # è·å–ç¨€ç–çŸ©é˜µçš„éé›¶å…ƒç´ çš„ä½ç½®å’Œå€¼
    coo = csr_matrix.tocoo()
    values = torch.tensor(coo.data, dtype=torch.float)
    #  indices = torch.tensor([coo.row, coo.col], dtype=torch.long)
    indices = torch.tensor(np.array([coo.row, coo.col]), dtype=torch.long)
    # åˆ›å»º torch.sparse_coo_tensor
    sparse_tensor = torch.sparse_coo_tensor(indices, values, size=csr_matrix.shape)
    return sparse_tensor


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--k_fold', type=int, default=10, help='k-fold cross validation')  #KæŠ˜äº¤å‰éªŒè¯ (é»˜è®¤: 10)
    parser.add_argument('--epochs', type=int, default=300, help='number of epochs to train')#1000->300 #è®­ç»ƒè½®æ•° (é»˜è®¤: 10)
    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate') #å­¦ä¹ ç‡ (Learning Rate, é»˜è®¤: 0.0001)
    parser.add_argument('--weight_decay', type=float, default=1e-3, help='weight_decay')#æƒé‡è¡°å‡ (é»˜è®¤: 0.001)
    parser.add_argument('--random_seed', type=int, default=1234, help='random seed')# éšæœºç§å­ (é»˜è®¤: 1234)
    parser.add_argument('--neighbor', type=int, default=20, help='neighbor') #é‚»å±…æ•°é‡ (é»˜è®¤: 20)
    parser.add_argument('--negative_rate', type=float, default=1.0, help='negative_rate') #è´Ÿé‡‡æ ·ç‡ (é»˜è®¤: 1.0)
    parser.add_argument('--dataset', default='C-dataset', help='dataset') #æ•°æ®é›†åç§° (é»˜è®¤: 'C-dataset')
    #  parser.add_argument('--dropout', default='0.2', type=float, help='dropout')
    #Graph Transformer (GT) å‚æ•°
    parser.add_argument('--gt_layer', default='2', type=int, help='graph transformer layer')#å±‚æ•° (é»˜è®¤: 2)
    parser.add_argument('--gt_head', default='2', type=int, help='graph transformer head')#æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 2)
    parser.add_argument('--gt_out_dim', default='200', type=int, help='graph transformer output dimension')#è¾“å‡ºç»´åº¦ (é»˜è®¤: 200)
    #Heterogeneous Graph Transformer (HGT) å‚æ•°
    parser.add_argument('--hgt_layer', default='2', type=int, help='heterogeneous graph transformer layer')#å±‚æ•° (é»˜è®¤: 2)
    parser.add_argument('--hgt_head', default='8', type=int, help='heterogeneous graph transformer head')#æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 8)
    parser.add_argument('--hgt_in_dim', default='64', type=int, help='heterogeneous graph transformer input dimension')#è¾“å…¥ç»´åº¦ (é»˜è®¤: 64)
    parser.add_argument('--hgt_head_dim', default='25', type=int, help='heterogeneous graph transformer head dimension')#æ¯ä¸ªå¤´çš„ç»´åº¦ (é»˜è®¤: 25)
    parser.add_argument('--hgt_out_dim', default='200', type=int,
                        help='heterogeneous graph transformer output dimension')#è¾“å‡ºç»´åº¦ (é»˜è®¤: 200)
    #Transformer (Tr) å‚æ•°     
    parser.add_argument('--tr_layer', default='2', type=int, help='transformer layer')#å±‚æ•° (é»˜è®¤: 2)
    parser.add_argument('--tr_head', default='4', type=int, help='transformer head')#æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 4)

    # æ·»åŠ SAITSå­æ¨¡å—çš„å‚æ•°,SAITS æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤„ç†åºåˆ—æ•°æ®ç¼ºå¤±å€¼ï¼ˆæ’è¡¥ï¼‰çš„ Transformer æ¶æ„æ¨¡å‹ã€‚
    parser.add_argument("--n_groups", type=int, default=2,
                        help="SAITS åˆ†ç»„æ•°ï¼ˆæ§åˆ¶æ¨¡å‹å¹¶è¡Œç»“æ„ï¼Œå½±å“ç‰¹å¾åˆ†ç»„å­¦ä¹ ï¼‰")#åˆ†ç»„æ•° (é»˜è®¤: 2)
    parser.add_argument("--n_group_inner_layers", type=int, default=2,
                        help="æ¯ç»„å†…çš„ Transformer å±‚æ•°ï¼ˆå†³å®šç‰¹å¾äº¤äº’æ·±åº¦ï¼‰")#æ¯ç»„å†…çš„ Transformer å±‚æ•° (é»˜è®¤: 2)
    #  parser.add_argument("--d_feature", type=int, default=128,
    #                    help="è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆéœ€ä¸ AMDGT è¾“å‡ºç‰¹å¾ç»´åº¦åŒ¹é…")

    parser.add_argument("--d_feature", type=int, default=400,
                        help="è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆéœ€ä¸ AMDGT è¾“å‡ºç‰¹å¾ç»´åº¦åŒ¹é…")

    #  parser.add_argument("--d_model", type=int, default=128,
    #                    help="Transformer æ¨¡å—çš„éšè—ç»´åº¦ï¼ˆéœ€ä¸ç‰¹å¾ç»´åº¦é€‚é…ï¼‰")

    parser.add_argument("--d_model", type=int, default=400,
                        help="Transformer æ¨¡å—çš„éšè—ç»´åº¦ï¼ˆéœ€ä¸ç‰¹å¾ç»´åº¦é€‚é…ï¼‰")

    parser.add_argument("--d_inner", type=int, default=512,
                        help="Transformer å‰é¦ˆç½‘ç»œçš„éšè—ç»´åº¦ï¼ˆå½±å“éçº¿æ€§å˜æ¢èƒ½åŠ›ï¼‰")
    parser.add_argument("--n_head", type=int, default=4,
                        help="Multi-Head Attention çš„å¤´æ•°ï¼ˆæ§åˆ¶æ³¨æ„åŠ›å¹¶è¡Œåº¦ï¼‰")

    # parser.add_argument("--d_k", type=int, default=32,
    #                    help="æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ key/value ç»´åº¦ï¼ˆéœ€æ»¡è¶³ d_model = n_head * d_kï¼‰")
    # parser.add_argument("--d_v", type=int, default=32,
    #                    help="æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ value ç»´åº¦ï¼ˆåŒ d_k é€»è¾‘ï¼‰")

    parser.add_argument("--d_k", type=int, default=100,
                        help="æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ key/value ç»´åº¦ï¼ˆéœ€æ»¡è¶³ d_model = n_head * d_kï¼‰")
    parser.add_argument("--d_v", type=int, default=100,
                        help="æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ value ç»´åº¦ï¼ˆåŒ d_k é€»è¾‘ï¼‰")

    # SAITS è®­ç»ƒä¸ä»»åŠ¡å‚æ•°
    #  parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--input_with_mask", action="store_true", default=True)#æ˜¯å¦å°†â€œæ©ç çŸ©é˜µâ€ï¼ˆMaskï¼‰ä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†å–‚ç»™æ¨¡å‹ã€‚True (é»˜è®¤)ï¼šæ¨¡å‹çœ‹åˆ°æ•°æ® [0.5, 0, 0.8] + æ©ç  [1, 0, 1]ã€‚
    parser.add_argument("--param_sharing_strategy", type=str, default="within_group",
                        choices=["within_group", "between_group"])#ï¼ˆç»„å†…å…±äº«ï¼Œç»„é—´å…±äº«ï¼‰æƒé‡
    parser.add_argument("--MIT", action="store_true", default=True)#æ©ç æ’è¡¥ä»»åŠ¡ï¼ŒSAITS èƒ½å¤Ÿâ€œè‡ªæˆ‘å­¦ä¹ â€çš„æ ¸å¿ƒå¼€å…³
    # åˆ†åˆ«ä¸º AMDGT å’Œ SAITS è®¾ç½® dropout
    parser.add_argument('--amdgt_dropout', type=float, default=0.2,
                        help='dropout rate for AMDGT components')#ä¸»æ¨¡å‹ AMDGTï¼ˆå›¾ç¥ç»ç½‘ç»œéƒ¨åˆ†ï¼ŒGT/HGTï¼‰ï¼Œéšæœºä¸¢å¼ƒç‡
    parser.add_argument('--saits_dropout', type=float, default=0.1,
                        help='dropout rate for SAITS components')# å­æ¨¡å— SAITSï¼ˆTransformeréƒ¨åˆ†ï¼‰

    args = parser.parse_args()
    args.data_dir = 'data/' + args.dataset + '/'
    args.result_dir = 'Result/' + args.dataset + '/AMNTDDA/'
    os.makedirs(args.result_dir, exist_ok=True)

    #  # === å¯ç”¨æ—¥å¿—è®°å½• ===
    # # 1. è·å–å½“å‰æ—¶é—´ï¼Œæ ¼å¼å¦‚ï¼š2023-10-27_15-30-00
    # # current_time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # # # 2. å®šä¹‰æ—¥å¿—æ–‡ä»¶å
    # # log_filename = os.path.join(args.result_dir, f'result_{current_time_str}.txt')
    # # # 3. é‡å®šå‘ print è¾“å‡º
    # # # ä»è¿™ä¸€è¡Œå¼€å§‹ï¼Œæ‰€æœ‰çš„ print() éƒ½ä¼šåŒæ—¶æ˜¾ç¤ºåœ¨å±å¹•å’Œå†™å…¥ txt æ–‡ä»¶
    # # sys.stdout = Logger(log_filename)
    # # === ã€ä¿®æ”¹ 2ã€‘: æ‰‹åŠ¨æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ç”¨äºä¿å­˜ç»“æœ ===
    # current_time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # result_file_path = os.path.join(args.result_dir, f'result_{current_time_str}.txt')
    # result_file = open(result_file_path, 'a', encoding='utf-8')
    # # å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä¸“é—¨ç”¨äºåŒæ—¶æ‰“å°åˆ°å±å¹• å¹¶ å†™å…¥æ–‡ä»¶
    # def log_result(content):
    #     print(content)  # æ‰“å°åˆ°å±å¹•
    #     result_file.write(content + '\n') # å†™å…¥æ–‡ä»¶
    #     result_file.flush() # ç«‹å³ä¿å­˜
    # print(f"æ—¥å¿—åŠŸèƒ½å·²å¯åŠ¨ï¼Œç»“æœå°†ä¿å­˜è‡³: {result_file_path}")
    # print(f"å½“å‰è¿è¡Œå‚æ•°: {args}")

    # -------------------------------------------------------
    # 1. å‡†å¤‡æ—¥å¿—æ–‡ä»¶
    # -------------------------------------------------------
    if not os.path.exists(args.result_dir):
        os.makedirs(args.result_dir)
        
    current_time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file_path = os.path.join(args.result_dir, f'result_{current_time_str}.txt')
    
    # æ‰“å¼€æ–‡ä»¶å¯¹è±¡ (è¿½åŠ æ¨¡å¼ 'a')
    f_log = open(log_file_path, 'a', encoding='utf-8')

    # å®šä¹‰ä¸€ä¸ªä¸“ç”¨å‡½æ•°ï¼šæ—¢æ‰“å°åˆ°å±å¹•ï¼Œåˆå†™å…¥æ–‡ä»¶
    def log_msg(content):
        print(content)  # å±å¹•æ˜¾ç¤º
        f_log.write(str(content) + '\n') # å†™å…¥æ–‡ä»¶
        f_log.flush()   # ç«‹å³ä¿å­˜ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒä¸¢å¤±æ•°æ®

    # è®°å½•å¿…è¦çš„å¯åŠ¨ä¿¡æ¯
    log_msg(f"Current Date: {datetime.now().strftime('%Y-%m-%d')}")
    log_msg(f"Log file path: {log_file_path}")
    log_msg(f"Parameters: {args}")
    # -------------------------------------------------------
    # 2. åŠ è½½æ•°æ® (è¿™ä¸€æ­¥å¯èƒ½æ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…)
    # -------------------------------------------------------
    print("æ­£åœ¨åŠ è½½æ•°æ®ï¼Œè¯·ç¨å€™...")  # è¿™è¡Œä¸éœ€è¦è¿›æ—¥å¿—ï¼Œç”¨æ™®é€š print
    
    data = get_data(args)
    args.drug_number = data['drug_number']
    args.disease_number = data['disease_number']
    args.protein_number = data['protein_number']

    data = data_processing(data, args)
    data = k_fold(data, args)
    # print("\n" + "="*30 + " Data ç»“æ„æ¦‚è§ˆ " + "="*30)
    # for key, value in data.items():
    #     # æƒ…å†µ1: å¦‚æœæ˜¯ PyTorch Tensor æˆ– Numpy æ•°ç»„ï¼Œæ‰“å°å½¢çŠ¶
    #     if hasattr(value, 'shape'): 
    #         print(f"[{key:<15}]: Shape {value.shape} | Type: {type(value).__name__}")
        
    #     # æƒ…å†µ2: å¦‚æœæ˜¯ List (é€šå¸¸å­˜ K-Fold çš„ç´¢å¼•)ï¼Œæ‰“å°é•¿åº¦å’Œå‰å‡ ä¸ªå…ƒç´ 
    #     elif isinstance(value, list):
    #         print(f"[{key:<15}]: List Len {len(value)} | First item: {value[0] if len(value)>0 else 'Empty'}")
        
    #     # æƒ…å†µ3: å…¶ä»–æ•°å€¼æˆ–å­—ç¬¦ä¸²
    #     else:
    #         print(f"[{key:<15}]: {value}")
    # print("="*75 + "\n")
    #drdr_graph è¯ç‰©åŒæ„å›¾   didi_graph (ç–¾ç—…åŒæ„å›¾)
    drdr_graph, didi_graph, data = dgl_similarity_graph(data, args)
    # ================= å…³é”®ä¿®æ”¹ï¼šå…ˆå®šä¹‰ç‰¹å¾å˜é‡ =================
      #å‡†å¤‡å…¬å¼7çš„Xè¾“å…¥h^(0) = W^0 xi+ b
    drug_feature = torch.FloatTensor(data['drugfeature']).to(device)# åµŒå…¥ç‰¹å¾
    disease_feature = torch.FloatTensor(data['diseasefeature']).to(device)# åµŒå…¥ç‰¹å¾
    protein_feature = torch.FloatTensor(data['proteinfeature']).to(device)
    all_sample = torch.tensor(data['all_drdi']).long()

    #-----add- dwp 2026-1-28---
    features_drug_struct = torch.FloatTensor(data['drf']).to(device)      # ç»“æ„ç‰¹å¾
    features_drug_func = torch.FloatTensor(data['drg']).to(device)         # åŠŸèƒ½ç‰¹å¾

    features_dis_pheno = torch.FloatTensor(data['dip']).to(device)         # è¡¨å‹ç‰¹å¾
    features_dis_func = torch.FloatTensor(data['dig']).to(device)          # åŠŸèƒ½ç‰¹å¾

    #--------------------------
    # æ–°å¢ï¼šä¸ºdrdr_graphå’Œdidi_graphç”ŸæˆHçŸ©é˜µ
    # H_drdr = generate_hypergraph_matrix(drdr_graph, k=1).to(device)
    # H_didi = generate_hypergraph_matrix(didi_graph, k=1).to(device)

    # --- æ–°ä»£ç  ---
    # æ³¨æ„ï¼šæˆ‘ä»¬è¦ç”¨â€œç‰¹å¾â€æ¥æ„å»ºï¼Œè€Œä¸æ˜¯ç”¨â€œå›¾â€æ¥æ„å»º
    # K_neigs: æ¯ä¸€ä¸ªèŠ‚ç‚¹æ‰¾å¤šå°‘ä¸ªé‚»å±…æ„æˆä¸€ä¸ªè¶…è¾¹ï¼Œå»ºè®®è®¾ä¸º 10-20
    # edge_type: å¦‚æœç‰¹å¾æ˜¯è¿ç»­æ•°å€¼ï¼Œ'euclid' é€šå¸¸è¾ƒå¥½ï¼›å¦‚æœæ˜¯è¡¨è¾¾è°±/æŒ‡çº¹ç›¸ä¼¼åº¦ï¼Œ'pearson' ä¹Ÿå¯ä»¥å°è¯•

    print("æ­£åœ¨é‡æ–°æ„å»ºè¯ç‰©è¶…å›¾ (åŸºäºç‰¹å¾)...")
    # print("drug_feature=",{drug_feature})
    #  # === åŠ å…¥è¿™ä¸€è¡Œæ£€æŸ¥å½¢çŠ¶ ===
    # print(f"drug_feature SHAPE: {drug_feature.shape}") 
    # # ========================

    # H_drdr = construct_hypergraph(drug_feature, 
    #                             K_neigs=[15],       # å‚æ•°å¯è°ƒï¼ŒscMHNNç”¨äº†70ï¼Œä½†å¯¹äºå°æ•°æ®é›†å»ºè®® 10-15
    #                             is_probH=True, 
    #                             m_prob=1.5,         # å¼€å¯æ¦‚ç‡æƒé‡ï¼Œæ•ˆæœé€šå¸¸ä¼˜äº 0/1
    #                             edge_type='euclid') # æˆ– 'pearson'
    # H_drdr = H_drdr.to(device) # åˆ«å¿˜äº†æ”¾åˆ° GPU ä¸Š

    # print("æ­£åœ¨é‡æ–°æ„å»ºç–¾ç—…è¶…å›¾ (åŸºäºç‰¹å¾)...")
    # H_didi = construct_hypergraph(disease_feature, 
    #                             K_neigs=[15], 
    #                             is_probH=True, 
    #                             m_prob=1.5,     
    #                             edge_type='euclid')
    # H_didi = H_didi.to(device)
    

   # === 2. æ„å»ºè¯ç‰©å¤šæ¨¡æ€è¶…å›¾ ===
   # ç»“æ„è¶…å›¾ (H_struct)
    H_drug_1 = construct_hypergraph(features_drug_struct, K_neigs=[10],is_probH=True,  m_prob=1.5, edge_type='pearson')
    # åŠŸèƒ½è¶…å›¾ (H_func)
    H_drug_2 = construct_hypergraph(features_drug_func, K_neigs=[10],is_probH=True,  m_prob=1.5, edge_type='euclid') 
    # è¯­ä¹‰è¶…å›¾ (H_embed)
    H_drug_3 = construct_hypergraph(drug_feature, K_neigs=[10],is_probH=True,  m_prob=1.5, edge_type='euclid')

    # èåˆè¯ç‰©è¶…å›¾ (æ‹¼æ¥åˆ—)
    # H_drug ç»´åº¦: [N_drug, (N_drug*3)]
    H_drug_final = Multi_omics_hyperedge_concat(H_drug_1, H_drug_2, H_drug_3)
     # ã€å…³é”®ã€‘ç§»åŠ¨åˆ° GPU
    H_drug_final = H_drug_final.to(device) 

    # === 3. æ„å»ºç–¾ç—…å¤šæ¨¡æ€è¶…å›¾ ===
    H_dis_1 = construct_hypergraph(features_dis_pheno, K_neigs=[10],is_probH=True,  m_prob=1.5, edge_type='pearson')
    H_dis_2 = construct_hypergraph(features_dis_func, K_neigs=[10], is_probH=True,  m_prob=1.5,edge_type='euclid')
    H_dis_3 = construct_hypergraph(disease_feature, K_neigs=[10], is_probH=True,  m_prob=1.5,edge_type='euclid')

    H_dis_final = Multi_omics_hyperedge_concat(H_dis_1, H_dis_2, H_dis_3)
    # ã€å…³é”®ã€‘ç§»åŠ¨åˆ° GPU
    H_dis_final = H_dis_final.to(device)
    # # === 4. ç”Ÿæˆ G çŸ©é˜µä¾›æ¨¡å‹ä½¿ç”¨ ===
    # G_drug = generate_G_from_H(H_drug_final)
    # G_dis = generate_G_from_H(H_dis_final)
    

    # æ‰“å° H_drdr å‰ 10 è¡Œï¼Œå‰ 10 åˆ—çš„æ•°å€¼
    # print("=== H_drdr å‰ 10x10 åŒºåŸŸæ•°å€¼ ===")
    # to_dense() å°†ç¨€ç–çŸ©é˜µè½¬ä¸ºæ™®é€šçŸ©é˜µ
    # print(H_drdr.to_dense()[:10, :10]) 
    # print(H_drdr)
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æƒé‡ï¼ˆå¦‚æœæ˜¯æ¦‚ç‡è¶…å›¾ï¼Œå€¼åº”è¯¥æ˜¯å°æ•°ï¼›å¦‚æœæ˜¯äºŒå€¼è¶…å›¾ï¼Œå€¼æ˜¯0æˆ–1ï¼‰
    # print("\n=== H_drdr æ ·æœ¬å€¼æ£€æŸ¥ ===")
    # æ‰“å°å‰20ä¸ªéé›¶å…ƒç´ çš„å€¼
    # print(H_drdr.values[:50])
    # # æ‰“å° H_drdr ä¸­æ‰€æœ‰é0ä¸”é1çš„ç‹¬ç‰¹å€¼
    # unique_values = torch.unique(H_drdr.values())
    # print("Unique weights in H:", unique_values)
    # éªŒè¯ä»£ç 
    # vals = H_drug_final.values()
    # print(f"æœ€å¤§æƒé‡: {vals.max().item():.4f}") # åº”è¯¥æ˜¯ 1.0 (è‡ªç¯)
    # print(f"æœ€å°æƒé‡: {vals.min().item():.4f}") # åº”è¯¥æ˜æ˜¾å°äº 1.0 (æ¯”å¦‚ 0.5, 0.7 ç­‰)
    # print(f"å¹³å‡æƒé‡: {vals.mean().item():.4f}") # åº”è¯¥åœ¨ 0.8-0.9 å·¦å³
    # print(f"å‰ 50 ä¸ªå€¼: {vals[:50]}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ (è°ƒè¯•ç”¨)
    print(f"è¯ç‰©èåˆè¶…å›¾ç»´åº¦: {H_drug_final.shape}, è¾¹æ•°: {H_drug_final._nnz()}")
    print(f"ç–¾ç—…èåˆè¶…å›¾ç»´åº¦: {H_dis_final.shape}, è¾¹æ•°: {H_dis_final._nnz()}")
    # æå–éé›¶æƒå€¼
    # 1. æ˜¾å¼åˆå¹¶ï¼ˆè¿™æ˜¯è§£å†³æŠ¥é”™çš„å…³é”®ï¼‰
    H_drug_final = H_drug_final.coalesce()
    vals = H_drug_final.values()

    if vals.numel() > 0:
        print(f"--- èåˆè¶…å›¾ç»Ÿè®¡ ---")
        print(f"è¶…è¾¹æ€»æ•°: {H_drug_final.shape[1]}")
        print(f"æœ€å¤§æƒé‡: {vals.max().item():.4f}") 
        print(f"æœ€å°æƒé‡: {vals.min().item():.4f}") 
        print(f"å¹³å‡æƒé‡: {vals.mean().item():.4f}") 
        # æ‰“å°å‰50ä¸ªï¼Œå¸¦ç‚¹æ ¼å¼çœ‹èµ·æ¥ä¸ä¹±
        print(f"å‰ 50 ä¸ªæƒé‡å€¼:\n{vals[:50].tolist()}")
    else:
        print("è­¦å‘Š: H_drug_final ä¸­æ²¡æœ‰éé›¶å€¼ï¼")
#   # 1. æ‰“å°å‰ 50 ä¸ªæ•°å€¼ (æ‰å¹³åŒ–)
#     print("G_drug å‰ 50 ä¸ªæ•°å€¼:")
#     print(G_drug.view(-1)[:50]) 

#     # 2. æˆ–è€…æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯ (æ¨èï¼Œæ›´èƒ½çœ‹å‡ºçŸ©é˜µæ˜¯å¦æœ‰é—®é¢˜)
#     print(f"G_drug å½¢çŠ¶: {G_drug.shape}")
#     print(f"G_drug æœ€å¤§å€¼: {G_drug.max().item()}")
#     print(f"G_drug æœ€å°å€¼: {G_drug.min().item()}")
#     print(f"G_drug å¹³å‡å€¼: {G_drug.mean().item()}")
# ----------------
    #------------------------------------------------------------
    drdr_graph = drdr_graph.to(device)
    didi_graph = didi_graph.to(device)
    # print("ç¬¬ä¸€ä¸ªå…³è”çŸ©é˜µæ˜¯")
    # print(H_drdr)
    # print("ç¬¬äºŒä¸ªå…³è”çŸ©é˜µæ˜¯")
    # print(H_didi)
    """
    ç¬¬ä¸€ä¸ªå…³è”çŸ©é˜µæ˜¯
    tensor(indices=tensor([[    0,     0,     0,  ...,   662,   662,   662],
                        [  790,  1121,  1364,  ..., 33497, 33690, 33812]]),
        values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),
       device='cuda:0', size=(663, 34155), nnz=66984, layout=torch.sparse_coo)
    ç¬¬äºŒä¸ªå…³è”çŸ©é˜µæ˜¯
    tensor(indices=tensor([[    0,     0,     0,  ...,   408,   408,   408],
                        [  119,   324,   560,  ..., 21442, 21480, 21654]]),
        values=tensor([1., 1., 1.,  ..., 1., 1., 2.]),
        device='cuda:0', size=(409, 22232), nnz=43646, layout=torch.sparse_coo)
    """
    # #å‡†å¤‡å…¬å¼7çš„Xè¾“å…¥h^(0) = W^0 xi+ b
    # drug_feature = torch.FloatTensor(data['drugfeature']).to(device)
    # disease_feature = torch.FloatTensor(data['diseasefeature']).to(device)
    # protein_feature = torch.FloatTensor(data['proteinfeature']).to(device)
    # all_sample = torch.tensor(data['all_drdi']).long()
    
    # print(torch.FloatTensor(data['drugfeature']))
    # print(torch.FloatTensor(data['diseasefeature']))
    # print(torch.FloatTensor(data['proteinfeature']))


    start = timeit.default_timer()#å¯åŠ¨è®¡æ—¶å™¨

    cross_entropy = nn.CrossEntropyLoss()#å®šä¹‰æŸå¤±å‡½æ•°-äº¤å‰ç†µæŸå¤±
    log_file_path = os.path.join(args.result_dir, 'training_metrics.txt')
    # Metric = ('Epoch\t\tTime\t\tAUC\t\tAUPR\t\tAccuracy\t\tPrecision\t\tRecall\t\tF1-score\t\tMcc')
  # === 3. æ›´æ–° Header å®šä¹‰ ===
    # Metric_Header = "Epoch\t\tTime\t\tLL\t\tAcc\t\tRMSE\t\tMAE\t\tRecall\t\tPrec\t\tF1\t\tAUC\t\tAUPRC\t\tSpec\t\tBrier\t\tTP\t\tFN\t\tFP\t\tTN\t\tPosAvg\t\tNegAvg"
    Metric_Header = (
        f"{'Epoch':<6}{'Time':<8}{'LL':<10}{'Acc':<10}{'RMSE':<10}{'MAE':<10}"
        f"{'Recall':<10}{'Prec':<10}{'F1':<10}{'AUC':<10}{'AUPRC':<10}{'Spec':<10}"
        f"{'Brier':<10}{'TP':<6}{'FN':<6}{'FP':<6}{'TN':<6}{'PosAvg':<10}{'NegAvg':<10}"
    )
    AUCs, AUPRs = [], []

    # print('Dataset:', args.dataset)
    log_msg(f'Dataset: {args.dataset}')
    for i in range(args.k_fold):
        # è®°å½•æŠ˜æ•°
        log_msg(f'\nFold: {i}')
        # è®°å½•è¡¨å¤´ï¼Œæ–¹ä¾¿åç»­å¤åˆ¶åˆ° Excel
        log_msg(Metric_Header)


        # print('fold:', i)
        # print(Metric)
        # print(Metric_Header) # æ›¿æ¢åŸæ¥çš„ print(Metric)

    #  # === åˆå§‹åŒ–æ¨¡å‹ (DualHyperGT) ===
    #     model = DualHyperGT(
    #         args=args,
    #         n_drug_nodes=drug_feature.shape[0],
    #         n_dis_nodes=disease_feature.shape[0],
    #         n_drug_hes=H_drug_final.shape[1],
    #         n_dis_hes=H_dis_final.shape[1],
    #         drug_in_channels=drug_feature.shape[1],
    #         dis_in_channels=disease_feature.shape[1],
    #         hidden_channels=64, # å¯è°ƒæ•´
    #         out_channels=64
    #     ).to(device)

        #model = AMNTDDA(args)
        #model = model.to(device)
        #optimizer = optim.Adam(model.parameters(), weight_decay=args.weight_decay, lr=args.lr)
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‡è®¾AMNTDDAéœ€è¦è¶…å›¾å‚æ•°å’ŒSAITSå‚æ•°ï¼‰
        model = AMNTDDA(args).to(device)

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optim.Adam(
            model.parameters(),#æ›´æ–°modelçš„å‚æ•°
            lr=args.lr,#å­¦ä¹ ç‡
            weight_decay=args.weight_decay#æƒé‡è¡°å‡
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºè¡°å‡å­¦ä¹ ç‡ï¼‰
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=50, verbose=True
        )#factor=0.5 å­¦ä¹ ç‡ç åŠï¼ˆä¹˜ä»¥ 0.5ï¼‰ï¼Œverbose=Trueæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æŸå¤±ä¸ä¸‹é™æ—¶æ‰“å°æç¤ºæ–‡å­—

        # å…³é”®ä¿®æ”¹ï¼šåˆ›å»ºå½“å‰æŠ˜çš„è®­ç»ƒé›†DataLoader
        X_train = torch.LongTensor(data['X_train'][i]).to(device)
        Y_train = torch.LongTensor(data['Y_train'][i]).to(device)
        # å°è£…ä¸ºTensorDataset
        #train_dataset = TensorDataset(X_train, Y_train)
        # ä»å®Œæ•´è®­ç»ƒé›†ä¸­åˆ’åˆ†20%ä½œä¸ºéªŒè¯é›†ï¼Œ80%ä½œä¸ºå®é™…è®­ç»ƒé›†
        val_size = int(0.2 * len(X_train))  # éªŒè¯é›†å¤§å°
        X_val = X_train[:val_size]  # éªŒè¯é›†æ ·æœ¬
        Y_val = Y_train[:val_size]
        X_train = X_train[val_size:]  # å®é™…è®­ç»ƒé›†æ ·æœ¬ï¼ˆå‰©ä½™80%ï¼‰
        Y_train = Y_train[val_size:]
        # åˆ›å»ºè®­ç»ƒé›†DataLoaderï¼ˆä½¿ç”¨åˆ’åˆ†åçš„è®­ç»ƒæ•°æ®ï¼‰
        train_dataset = TensorDataset(X_train, Y_train)
        data['train_loader'] = DataLoader(
            train_dataset,
            batch_size=64,  # å¯æ ¹æ®éœ€æ±‚è°ƒæ•´
            shuffle=True,# ã€å…³é”®ã€‘æ¯ä¸ªEpochå¼€å§‹æ—¶æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼Ÿå¿…é¡»æ˜¯ Trueï¼
            drop_last=False # å¦‚æœæœ€åå‰©çš„æ•°æ®ä¸å¤Ÿ64æ¡ï¼Œæ˜¯å¦ä¸¢å¼ƒï¼ŸFalseè¡¨ç¤ºä¿ç•™ã€‚
        )

        # å¼ºåˆ¶åˆ›å»ºéªŒè¯é›†DataLoaderï¼ˆä¸å†ä¾èµ–dataä¸­æ˜¯å¦æœ‰'X_val'/'Y_val'ï¼‰
        val_dataset = TensorDataset(X_val, Y_val)
        data['val_loader'] = DataLoader(
            val_dataset,
            batch_size=64, # æ¯æ¬¡å–‚ç»™æ¨¡å‹ 64 æ¡æ•°æ®
            shuffle=False, 
            drop_last=False # å¦‚æœæœ€åå‰©çš„æ•°æ®ä¸å¤Ÿ64æ¡ï¼Œæ˜¯å¦ä¸¢å¼ƒï¼ŸFalseè¡¨ç¤ºä¿ç•™ã€‚
        )

        # å‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†åŠ è½½å™¨ï¼ˆå‡è®¾dataåŒ…å«kæŠ˜åˆ’åˆ†çš„è®­ç»ƒ/éªŒè¯æ•°æ®ï¼‰
        best_val_loss = float('inf')#è®¾å®šä¸ºæ­£æ— ç©·å¤§
        best_model_path = os.path.join(args.result_dir, 'best_model.pth')#ä¿å­˜æœ€ä½³è®­ç»ƒæ¨¡å‹
        #add dwp
        counter = 0
        patience = 20 # Early stopping patience
        
        best_metrics_str = ""  # è®°å½•æœ€ä½³AUCæ—¶çš„å„ä¸ªæŒ‡æ ‡#add dwp
        best_auc, best_aupr, best_accuracy, best_precision, best_recall, best_f1, best_mcc = 0, 0, 0, 0, 0, 0, 0
        X_train = torch.LongTensor(data['X_train'][i]).to(device)#é‡æ–°åŠ è½½å®Œæ•´è®­ç»ƒé›†
        Y_train = torch.LongTensor(data['Y_train'][i]).to(device)

        X_test = torch.LongTensor(data['X_test'][i]).to(device)#åŠ è½½æµ‹è¯•é›†çš„ç´¢å¼•ï¼Œæ¬è¿åˆ° GPU
        Y_test = data['Y_test'][i].flatten()#å‹ç¼©ç»´åº¦ï¼ŒScikit-learn è®¡ç®— AUC/AUPR çš„å‡½æ•°é€šå¸¸è¦æ±‚æ ‡ç­¾æ˜¯ä¸€ç»´æ•°ç»„
        # print(data['Y_test'][i])

        drdipr_graph, data = dgl_heterograph(data, data['X_train'][i], args)
        drdipr_graph = drdipr_graph.to(device)

        for epoch in range(args.epochs):
            #model.train()
            #train_loss, train_mae = train(args, model, data['train_loader'], optimizer, epoch)
            # éªŒè¯ï¼ˆå‡è®¾data['val_loader']æ˜¯å½“å‰æŠ˜çš„éªŒè¯é›†ï¼‰
            #val_loss, val_mae = validate(args, model, data['val_loader'])
            train_loss, train_mae = train(
                args, model, data['train_loader'], optimizer, epoch,
                drdr_graph, didi_graph, drdipr_graph,  # å›¾æ•°æ®
                drug_feature, disease_feature, protein_feature,  # å…¨å±€ç‰¹å¾
                H_drug_final, H_dis_final # ä¼ å…¥æ–°æ„å»ºçš„è¶…å›¾
            )
            # éªŒè¯å‡½æ•°åŒæ ·è¡¥å……å‚æ•°
            val_loss, val_mae = validate(
                args, model, data['val_loader'],
                drdr_graph, didi_graph, drdipr_graph,
                drug_feature, disease_feature, protein_feature,
                H_drug_final, H_dis_final
            )

            # è°ƒæ•´å­¦ä¹ ç‡
            scheduler.step(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss
                }, best_model_path)
                print(f"Saved best model at epoch {epoch}, Val Loss: {best_val_loss:.4f}")
            else:
                counter += 1 # <--- é‡è¦ï¼šå¦‚æœæ²¡è¿›æ­¥ï¼Œè®¡æ•°å™¨+1
                print(f"No improvement. Counter: {counter}/{patience}")
            
            # --- 3. åŠ å…¥ä¸­æ–­é€»è¾‘ ---
            if counter >= patience:
                # print(f"Early stopping triggered at epoch {epoch}!")
                log_msg(f"Early stopping triggered at epoch {epoch}!") # å»ºè®®æŠŠæ—©åœä¿¡æ¯å†™å…¥æ–‡ä»¶
                break  # <--- é‡è¦ï¼šå¼ºåˆ¶è·³å‡ºå¾ªç¯


            with torch.no_grad():
                model.eval()# 1. ä½¿ç”¨ test_logits æ¥æ”¶æ¨¡å‹åŸå§‹è¾“å‡ºï¼Œç»å¯¹ä¸è¦è¦†ç›–å®ƒ
                d_loss, di_loss, test_raw_logits= model(drdr_graph, didi_graph, drdipr_graph, drug_feature,
                                                      disease_feature, protein_feature, X_test, H_drug_final, H_dis_final)

            
            # 3. å…ˆè®¡ç®—æ¦‚ç‡ (Softmax éœ€è¦ Tensor æ ¼å¼çš„ Logits)
            test_prob_tensor = fn.softmax(test_raw_logits, dim=-1)# å°†å¾—åˆ†è½¬ä¸ºæ¦‚ç‡ (0~1)
            # 4. å†è®¡ç®—é¢„æµ‹ç±»åˆ« (Argmax)
            # test_score = torch.argmax(test_logits, dim=-1)#è½¬ç±»åˆ«
            test_pred_tensor = torch.argmax(test_raw_logits, dim=-1)#
            #  ç»Ÿä¸€è½¬ä¸º Numpyï¼Œå‡†å¤‡ä¼ ç»™ get_metric
            # å–å‡ºæ­£æ ·æœ¬(ç´¢å¼•1)çš„æ¦‚ç‡ï¼Œå¹¶è½¬åˆ° CPU
            test_prob = test_prob_tensor[:, 1].cpu().numpy()# åªå–â€œæ­£æ ·æœ¬(1)â€çš„æ¦‚ç‡
            test_score = test_pred_tensor.cpu().numpy()
            # test_prob = test_prob.cpu().numpy()# è½¬ä¸ºnumpyæ•°ç»„

            # test_score = test_score.cpu().numpy()
            # # y_true_np: çœŸå®æ ‡ç­¾ (ç¡®ä¿å®ƒä¹Ÿæ˜¯numpyæ•°ç»„)
            # y_true_np = Y_test

            # # 4. ç»Ÿä¸€è½¬ä¸º Numpy
            # # å–å‡ºæ­£ç±»(1)çš„æ¦‚ç‡
            # y_prob_np = test_prob_tensor[:, 1].cpu().numpy()
            # # å–å‡ºé¢„æµ‹ç±»åˆ«
            # y_pred_np = test_pred_tensor.cpu().numpy()
            # # çœŸå®æ ‡ç­¾
            # y_true_np = Y_test
             
            # 4. è°ƒç”¨ metric.py è®¡ç®—æŒ‡æ ‡
            # AUC, AUPR, accuracy, precision, recall, f1, mcc = get_metric(Y_test, test_score, test_prob)
            AUC, AUPR, accuracy, precision, recall, f1, mcc, ll, rmse, mae, specificity, brier, tp, fn_count, fp, tn, pos_avg, neg_avg = get_metric(Y_test, test_score, test_prob)

            end = timeit.default_timer()
            time = end - start
            # show = [epoch + 1, round(time, 2), round(AUC, 5), round(AUPR, 5), round(accuracy, 5),
            #         round(precision, 5), round(recall, 5), round(f1, 5), round(mcc, 5)]
            
            # å®šä¹‰è¦æ‰“å°çš„æ•°æ®åˆ—è¡¨ (é¡ºåºéœ€å¯¹åº”è¡¨å¤´)
            # è¡¨å¤´: Epoch, LL, Acc, RMSE, MAE, Recall, Precision, F1, AUC, AUPRC, Specificity, BrierScore, TP, FN, FP, TN, PosAvg, NegAvg
            show = [
                epoch + 1, round(time, 2),round(ll, 5),round(accuracy, 5),round(rmse, 5),
                round(mae, 5),round(recall, 5),round(precision, 5),round(f1, 5),round(AUC, 5),
                round(AUPR, 5),round(specificity, 5),round(brier, 5),tp, fn_count, fp, tn,
                round(pos_avg, 5),round(neg_avg, 5)]
            print('\t\t'.join(map(str, show)))
            #================
             # æ‹¼æ¥æˆå­—ç¬¦ä¸²
            # metrics_str = '\t\t'.join(map(str, show))
            metrics_str = (
                f"{epoch + 1:<6}"
                f"{time:<8.2f}"
                f"{ll:<10.5f}"
                f"{accuracy:<10.5f}"
                f"{rmse:<10.5f}"
                f"{mae:<10.5f}"
                f"{recall:<10.5f}"
                f"{precision:<10.5f}"
                f"{f1:<10.5f}"
                f"{AUC:<10.5f}"
                f"{AUPR:<10.5f}"
                f"{specificity:<10.5f}"
                f"{brier:<10.5f}"
                f"{tp:<6}"
                f"{fn_count:<6}"
                f"{fp:<6}"
                f"{tn:<6}"
                f"{pos_avg:<10.5f}"
                f"{neg_avg:<10.5f}"
            )
            # å…³é”®ï¼šè°ƒç”¨ log_result å†™å…¥æ–‡ä»¶
            log_msg(metrics_str)
            #================
            if AUC > best_auc:
                best_epoch = epoch + 1
                best_auc = AUC
                # best_aupr, best_accuracy, best_precision, best_recall, best_f1, best_mcc = AUPR, accuracy, precision, recall, f1, mcc
                # è®°å½•æœ€ä½³æ—¶åˆ»çš„æ‰€æœ‰æŒ‡æ ‡å­—ç¬¦ä¸²ï¼Œç”¨äºæœ€åå±•ç¤º
                best_aupr, best_accuracy, best_precision, best_recall, best_f1, best_mcc = AUPR, accuracy, precision, recall, f1, mcc
                # print('AUC improved at epoch ', best_epoch, ';\tbest_auc:', best_auc)
                # è®°å½•æœ€ä½³AUCä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆè¿™ä¹Ÿæ˜¯â€œå¿…è¦å†…å®¹â€ï¼‰
                log_msg(f'AUC improved at epoch  {best_epoch} ;\tbest_auc: {best_auc}')
        AUCs.append(best_auc)
        AUPRs.append(best_aupr)

    # print('AUC:', AUCs)
    # AUC_mean = np.mean(AUCs)
    # AUC_std = np.std(AUCs)
    # print('Mean AUC:', AUC_mean, '(', AUC_std, ')')

    # print('AUPR:', AUPRs)
    # AUPR_mean = np.mean(AUPRs)
    # AUPR_std = np.std(AUPRs)
    # print('Mean AUPR:', AUPR_mean, '(', AUPR_std, ')')
        # === ã€ä¿®æ”¹ 5ã€‘: è®°å½•æœ€åçš„å¹³å‡ç»“æœ ===
    log_msg(f'AUC: {AUCs}')
    AUC_mean = np.mean(AUCs)
    AUC_std = np.std(AUCs)
    log_msg(f'Mean AUC: {AUC_mean} ( {AUC_std} )')

    log_msg(f'AUPR: {AUPRs}')
    AUPR_mean = np.mean(AUPRs)
    AUPR_std = np.std(AUPRs)
    log_msg(f'Mean AUPR: {AUPR_mean} ( {AUPR_std} )')
    
    # è®°å¾—æœ€åå…³é—­æ–‡ä»¶
    f_log.close()
               # === Debugging Replacement Block ===
            


